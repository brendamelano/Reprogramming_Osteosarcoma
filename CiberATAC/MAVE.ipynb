{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b07769",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d559428a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/Users/brendamelano/opt/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/warnings.py:53: LightningDeprecationWarning: pytorch_lightning.utilities.warnings.rank_zero_deprecation has been deprecated in v1.6 and will be removed in v1.8. Use the equivalent function from the pytorch_lightning.utilities.rank_zero module instead.\n",
      "  new_rank_zero_deprecation(\n",
      "/Users/brendamelano/opt/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/warnings.py:58: LightningDeprecationWarning: The `pytorch_lightning.loggers.base.rank_zero_experiment` is deprecated in v1.7 and will be removed in v1.9. Please use `pytorch_lightning.loggers.logger.rank_zero_experiment` instead.\n",
      "  return new_rank_zero_deprecation(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import anndata as AnnData\n",
    "import scrublet as scr\n",
    "import cellrank as cr\n",
    "import seaborn as sns\n",
    "import scvelo as scv\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import networkx\n",
    "import fsspec\n",
    "import igraph\n",
    "import scvi\n",
    "import desc \n",
    "import umap\n",
    "import h5py\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "311dfe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Iterable\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.distributions import Normal\n",
    "import warnings\n",
    "from typing import Optional, Tuple, Union\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Distribution, Gamma, Poisson, constraints\n",
    "from torch.distributions.utils import (\n",
    "    broadcast_all,\n",
    "    lazy_property,\n",
    "    logits_to_probs,\n",
    "    probs_to_logits,\n",
    ")\n",
    "\n",
    "\n",
    "def make_labels_ccle(metapath, expar, barcodes):\n",
    "    metadf = pd.read_csv(metapath, sep=\"\\t\", index_col=0)\n",
    "    if \"Site_Primary\" in metadf.columns:\n",
    "        metadf[\"CellType\"] = metadf[\"Site_Primary\"]\n",
    "        metadf[\"Barcode\"] = metadf.index\n",
    "    classes = np.unique(list(metadf[\"CellType\"]))\n",
    "    classes = np.array(\n",
    "        [each for each in classes if \"nan\" not in each])\n",
    "    metadf = metadf[metadf[\"CellType\"].isin(classes)]\n",
    "    metadf = metadf[metadf[\"Barcode\"].isin(barcodes)]\n",
    "    new_barcodes, idx_1, idx_2 = np.intersect1d(\n",
    "        barcodes, np.array(metadf[\"Barcode\"]),\n",
    "        return_indices=True)\n",
    "    outar = expar[idx_1, :]\n",
    "    outdf = metadf.iloc[idx_2, :]\n",
    "    out_barcodes = np.array(barcodes, dtype=\"|U64\")[idx_1]\n",
    "    one_hot = pd.get_dummies(outdf[\"CellType\"])\n",
    "    one_hot_tensor = torch.from_numpy(np.array(one_hot))\n",
    "    return outar, outdf, out_barcodes, one_hot_tensor\n",
    "\n",
    "\n",
    "def one_hot(index, n_cat):\n",
    "    onehot = torch.zeros(index.size(0), n_cat, device=index.device)\n",
    "    onehot.scatter_(1, index.type(torch.long), 1)\n",
    "    return onehot.type(torch.float32)\n",
    "\n",
    "\n",
    "def reparameterize_gaussian(mu, var):\n",
    "    return Normal(mu, var.sqrt()).rsample()\n",
    "\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "class CustomConnected(nn.Module):\n",
    "    def __init__(self, inputsize, hiddensize, connections):\n",
    "        super().__init__()\n",
    "        self.inputsize = inputsize\n",
    "        self.hiddensize = hiddensize\n",
    "        # Connections in TF x Gene (binary)\n",
    "        self.connections = connections\n",
    "        # Weights in TF x Gene dimension\n",
    "        weights = torch.Tensor(self.hiddensize, self.inputsize)\n",
    "        self.weights = nn.Parameter(weights)\n",
    "        bias = torch.Tensor(self.hiddensize)\n",
    "        self.bias = nn.Parameter(bias)\n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_uniform_(\n",
    "            self.weights, a=math.sqrt(5),\n",
    "            nonlinearity='leaky_relu')\n",
    "        # Initialize bias with union distribution\n",
    "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n",
    "\n",
    "    def forward(self, x):\n",
    "        enforced_weights = torch.mul(\n",
    "            self.weights, self.connections.detach())\n",
    "        ew_times_x = torch.mm(x, enforced_weights.detach().t())\n",
    "        return torch.add(ew_times_x, self.bias)\n",
    "\n",
    "\n",
    "class FCLayersEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper class to build fully-connected layers for a neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_in\n",
    "        The dimensionality of the input\n",
    "    n_out\n",
    "        The dimensionality of the output\n",
    "    n_cat_list\n",
    "        A list containing, for each category of interest,\n",
    "        the number of categories. Each category will be\n",
    "        included using a one-hot encoding.\n",
    "    n_layers\n",
    "        The number of fully-connected hidden layers\n",
    "    n_hidden\n",
    "        The number of nodes per hidden layer\n",
    "    connections\n",
    "        A boolean tensor indeicating weights\n",
    "        to set to zero\n",
    "    dropout_rate\n",
    "        Dropout rate to apply to each of the hidden layers\n",
    "    use_batch_norm\n",
    "        Whether to have `BatchNorm` layers or not\n",
    "    use_layer_norm\n",
    "        Whether to have `LayerNorm` layers or not\n",
    "    use_activation\n",
    "        Whether to have layer activation or not\n",
    "    bias\n",
    "        Whether to learn bias in linear layers or not\n",
    "    inject_covariates\n",
    "        Whether to inject covariates in each layer,\n",
    "        or just the first (default).\n",
    "    activation_fn\n",
    "        Which activation function to use\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in: int,\n",
    "        n_out: int,\n",
    "        n_cat_list: Iterable[int] = None,\n",
    "        n_layers: int = 1,\n",
    "        n_hidden: int = 128,\n",
    "        connections=None,\n",
    "        dropout_rate: float = 0.1,\n",
    "        use_batch_norm: bool = True,\n",
    "        use_layer_norm: bool = False,\n",
    "        use_activation: bool = True,\n",
    "        bias: bool = True,\n",
    "        inject_covariates: bool = True,\n",
    "        activation_fn: nn.Module = nn.ReLU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        connect_dim2 = n_in\n",
    "        if n_cat_list is not None:\n",
    "            connect_dim2 += n_cat_list[0]\n",
    "        if connections is None:\n",
    "            connections = torch.ones(n_hidden, connect_dim2).long().to(device)\n",
    "        else:\n",
    "            if connections.shape[1] < connect_dim2:\n",
    "                print(\"Appending connections\")\n",
    "                temp_tensor = torch.zeros(n_hidden, connect_dim2 - n_in).long().to(device)\n",
    "                connections = torch.cat((connections, temp_tensor), axis=-1)\n",
    "\n",
    "        self.connections = connections\n",
    "        self.inject_covariates = inject_covariates\n",
    "        layers_dim = [n_in] + (n_layers - 1) * [n_hidden] + [n_out]\n",
    "\n",
    "        if n_cat_list is not None:\n",
    "            # n_cat = 1 will be ignored\n",
    "            self.n_cat_list = [\n",
    "                n_cat if n_cat > 1 else 0 for n_cat in n_cat_list]\n",
    "        else:\n",
    "            self.n_cat_list = []\n",
    "\n",
    "        cat_dim = sum(self.n_cat_list)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            collections.OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        \"Layer_{}\".format(i),\n",
    "                        nn.Sequential(\n",
    "                            CustomConnected(\n",
    "                                n_in + cat_dim * self.inject_into_layer(i),\n",
    "                                n_out,\n",
    "                                connections\n",
    "                            ),\n",
    "                            # non-default params come from defaults in\n",
    "                            # original Tensorflow implementation\n",
    "                            nn.BatchNorm1d(n_out, momentum=0.01, eps=0.001)\n",
    "                            if use_batch_norm\n",
    "                            else None,\n",
    "                            nn.LayerNorm(n_out, elementwise_affine=False)\n",
    "                            if use_layer_norm\n",
    "                            else None,\n",
    "                            activation_fn() if use_activation else None,\n",
    "                            nn.Dropout(\n",
    "                                p=dropout_rate) if dropout_rate > 0 else None,\n",
    "                        ),\n",
    "                    )\n",
    "                    for i, (n_in, n_out) in enumerate(\n",
    "                        zip(layers_dim[:-1], layers_dim[1:])\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def inject_into_layer(self, layer_num) -> bool:\n",
    "        \"\"\"Helper to determine if covariates should be injected.\"\"\"\n",
    "        user_cond = layer_num == 0 or (\n",
    "            layer_num > 0 and self.inject_covariates)\n",
    "        return user_cond\n",
    "\n",
    "    def set_online_update_hooks(self, hook_first_layer=True):\n",
    "        self.hooks = []\n",
    "\n",
    "        def _hook_fn_weight(grad):\n",
    "            categorical_dims = sum(self.n_cat_list)\n",
    "            new_grad = torch.zeros_like(grad)\n",
    "            if categorical_dims > 0:\n",
    "                new_grad[:, -categorical_dims:] = grad[:, -categorical_dims:]\n",
    "            return new_grad\n",
    "\n",
    "        def _hook_fn_zero_out(grad):\n",
    "            return grad * 0\n",
    "\n",
    "        for i, layers in enumerate(self.fc_layers):\n",
    "            # if i > 0 and not self.inject_covariates:\n",
    "            #     break\n",
    "            for layer in layers:\n",
    "                if i == 0 and not hook_first_layer:\n",
    "                    continue\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    if self.inject_into_layer(i):\n",
    "                        w = layer.weight.register_hook(_hook_fn_weight)\n",
    "                    else:\n",
    "                        w = layer.weight.register_hook(_hook_fn_zero_out)\n",
    "                    self.hooks.append(w)\n",
    "                    b = layer.bias.register_hook(_hook_fn_zero_out)\n",
    "                    self.hooks.append(b)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, batch_index):\n",
    "        \"\"\"\n",
    "        Forward computation on ``x``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x\n",
    "            tensor of values with shape ``(n_in,)``\n",
    "        batch_index\n",
    "            tensor of batch membership(s)\n",
    "        x: torch.Tensor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        py:class:`torch.Tensor`\n",
    "            tensor of shape ``(n_out,)``\n",
    "\n",
    "        \"\"\"\n",
    "        cat_list = [batch_index]\n",
    "        one_hot_cat_list = []\n",
    "        # for generality in this list many indices useless.\n",
    "\n",
    "        if len(self.n_cat_list) > len(cat_list):\n",
    "            print(self.n_cat_list)\n",
    "            print(cat_list)\n",
    "            raise ValueError(\n",
    "                \"nb. categorical args provided doesn't match init. params.\"\n",
    "            )\n",
    "        for n_cat, cat in zip(self.n_cat_list, cat_list):\n",
    "            if n_cat and cat is None:\n",
    "                raise ValueError(\n",
    "                    \"cat not provided while n_cat != 0 in init. params.\")\n",
    "            # n_cat = 1 will be ignored - no additional information\n",
    "            if n_cat > 1:\n",
    "                if cat.size(1) != n_cat:\n",
    "                    one_hot_cat = one_hot(cat, n_cat)\n",
    "                else:\n",
    "                    one_hot_cat = cat  # cat has already been one_hot encoded\n",
    "                one_hot_cat_list += [one_hot_cat]\n",
    "        for i, layers in enumerate(self.fc_layers):\n",
    "            for layer in layers:\n",
    "                if layer is not None:\n",
    "                    if isinstance(layer, nn.BatchNorm1d):\n",
    "                        if x.dim() == 3:\n",
    "                            x = torch.cat(\n",
    "                                [(layer(slice_x)).unsqueeze(0)\n",
    "                                 for slice_x in x], dim=0\n",
    "                            )\n",
    "                        else:\n",
    "                            x = layer(x)\n",
    "                    else:\n",
    "                        if isinstance(layer, nn.Linear) or\\\n",
    "                                isinstance(layer, CustomConnected) and\\\n",
    "                                self.inject_into_layer(i):\n",
    "                            if x.dim() == 3:\n",
    "                                one_hot_cat_list_layer = [\n",
    "                                    o.unsqueeze(0).expand(\n",
    "                                        (x.size(0), o.size(0), o.size(1))\n",
    "                                    )\n",
    "                                    for o in one_hot_cat_list\n",
    "                                ]\n",
    "                            else:\n",
    "                                one_hot_cat_list_layer = one_hot_cat_list\n",
    "                            x = torch.cat((x, *one_hot_cat_list_layer), dim=-1)\n",
    "                        x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FCLayers(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper class to build fully-connected layers for a neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_in\n",
    "        The dimensionality of the input\n",
    "    n_out\n",
    "        The dimensionality of the output\n",
    "    n_cat_list\n",
    "        A list containing, for each category of interest,\n",
    "        the number of categories. Each category will be\n",
    "        included using a one-hot encoding.\n",
    "    n_layers\n",
    "        The number of fully-connected hidden layers\n",
    "    n_hidden\n",
    "        The number of nodes per hidden layer\n",
    "    dropout_rate\n",
    "        Dropout rate to apply to each of the hidden layers\n",
    "    use_batch_norm\n",
    "        Whether to have `BatchNorm` layers or not\n",
    "    use_layer_norm\n",
    "        Whether to have `LayerNorm` layers or not\n",
    "    use_activation\n",
    "        Whether to have layer activation or not\n",
    "    bias\n",
    "        Whether to learn bias in linear layers or not\n",
    "    inject_covariates\n",
    "        Whether to inject covariates in each\n",
    "        layer, or just the first (default).\n",
    "    activation_fn\n",
    "        Which activation function to use\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in: int,\n",
    "        n_out: int,\n",
    "        n_cat_list: Iterable[int] = None,\n",
    "        n_layers: int = 1,\n",
    "        n_hidden: int = 128,\n",
    "        dropout_rate: float = 0.1,\n",
    "        use_batch_norm: bool = True,\n",
    "        use_layer_norm: bool = False,\n",
    "        use_activation: bool = True,\n",
    "        bias: bool = True,\n",
    "        inject_covariates: bool = True,\n",
    "        activation_fn: nn.Module = nn.ReLU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.inject_covariates = inject_covariates\n",
    "        layers_dim = [n_in] + (n_layers - 1) * [n_hidden] + [n_out]\n",
    "\n",
    "        if n_cat_list is not None:\n",
    "            # n_cat = 1 will be ignored\n",
    "            self.n_cat_list = [\n",
    "                n_cat if n_cat > 1 else 0 for n_cat in n_cat_list]\n",
    "        else:\n",
    "            self.n_cat_list = []\n",
    "\n",
    "        cat_dim = sum(self.n_cat_list)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            collections.OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        \"Layer_{}\".format(i),\n",
    "                        nn.Sequential(\n",
    "                            nn.Linear(\n",
    "                                n_in + cat_dim * self.inject_into_layer(i),\n",
    "                                n_out,\n",
    "                                bias=bias,\n",
    "                            ),\n",
    "                            # non-default params come from defaults in\n",
    "                            # original Tensorflow implementation\n",
    "                            nn.BatchNorm1d(n_out, momentum=0.01, eps=0.001)\n",
    "                            if use_batch_norm\n",
    "                            else None,\n",
    "                            nn.LayerNorm(n_out, elementwise_affine=False)\n",
    "                            if use_layer_norm\n",
    "                            else None,\n",
    "                            activation_fn() if use_activation else None,\n",
    "                            nn.Dropout(\n",
    "                                p=dropout_rate) if dropout_rate > 0 else None,\n",
    "                        ),\n",
    "                    )\n",
    "                    for i, (n_in, n_out) in enumerate(\n",
    "                        zip(layers_dim[:-1], layers_dim[1:])\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def inject_into_layer(self, layer_num) -> bool:\n",
    "        \"\"\"Helper to determine if covariates should be injected.\"\"\"\n",
    "        user_cond = layer_num == 0 or\\\n",
    "            (layer_num > 0 and self.inject_covariates)\n",
    "        return user_cond\n",
    "\n",
    "    def set_online_update_hooks(self, hook_first_layer=True):\n",
    "        self.hooks = []\n",
    "\n",
    "        def _hook_fn_weight(grad):\n",
    "            categorical_dims = sum(self.n_cat_list)\n",
    "            new_grad = torch.zeros_like(grad)\n",
    "            if categorical_dims > 0:\n",
    "                new_grad[:, -categorical_dims:] = grad[:, -categorical_dims:]\n",
    "            return new_grad\n",
    "\n",
    "        def _hook_fn_zero_out(grad):\n",
    "            return grad * 0\n",
    "\n",
    "        for i, layers in enumerate(self.fc_layers):\n",
    "            # if i > 0 and not self.inject_covariates:\n",
    "            #     break\n",
    "            for layer in layers:\n",
    "                if i == 0 and not hook_first_layer:\n",
    "                    continue\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    if self.inject_into_layer(i):\n",
    "                        w = layer.weight.register_hook(_hook_fn_weight)\n",
    "                    else:\n",
    "                        w = layer.weight.register_hook(_hook_fn_zero_out)\n",
    "                    self.hooks.append(w)\n",
    "                    b = layer.bias.register_hook(_hook_fn_zero_out)\n",
    "                    self.hooks.append(b)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, *cat_list: int):\n",
    "        \"\"\"\n",
    "        Forward computation on ``x``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x\n",
    "            tensor of values with shape ``(n_in,)``\n",
    "        cat_list\n",
    "            list of category membership(s) for this sample\n",
    "        x: torch.Tensor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        py:class:`torch.Tensor`\n",
    "            tensor of shape ``(n_out,)``\n",
    "\n",
    "        \"\"\"\n",
    "        # for generality in this list many indices useless\n",
    "        one_hot_cat_list = []\n",
    "\n",
    "        if len(self.n_cat_list) > len(cat_list):\n",
    "            print(self.n_cat_list)\n",
    "            print(cat_list)\n",
    "            raise ValueError(\n",
    "                \"nb. categorical args provided doesn't match init. params.\"\n",
    "            )\n",
    "        for n_cat, cat in zip(self.n_cat_list, cat_list):\n",
    "            if n_cat and cat is None:\n",
    "                raise ValueError(\n",
    "                    \"cat not provided while n_cat != 0 in init. params.\")\n",
    "            # n_cat = 1 will be ignored - no additional information\n",
    "            if n_cat > 1:\n",
    "                if cat.size(1) != n_cat:\n",
    "                    one_hot_cat = one_hot(cat, n_cat)\n",
    "                else:\n",
    "                    one_hot_cat = cat  # cat has already been one_hot encoded\n",
    "                one_hot_cat_list += [one_hot_cat]\n",
    "        for i, layers in enumerate(self.fc_layers):\n",
    "            for layer in layers:\n",
    "                if layer is not None:\n",
    "                    if isinstance(layer, nn.BatchNorm1d):\n",
    "                        if x.dim() == 3:\n",
    "                            x = torch.cat(\n",
    "                                [(layer(slice_x)).unsqueeze(0)\n",
    "                                 for slice_x in x], dim=0\n",
    "                            )\n",
    "                        else:\n",
    "                            x = layer(x)\n",
    "                    else:\n",
    "                        if isinstance(layer, nn.Linear) and\\\n",
    "                                self.inject_into_layer(i):\n",
    "                            if x.dim() == 3:\n",
    "                                one_hot_cat_list_layer = [\n",
    "                                    o.unsqueeze(0).expand(\n",
    "                                        (x.size(0), o.size(0), o.size(1))\n",
    "                                    )\n",
    "                                    for o in one_hot_cat_list\n",
    "                                ]\n",
    "                            else:\n",
    "                                one_hot_cat_list_layer = one_hot_cat_list\n",
    "                            x = torch.cat((x, *one_hot_cat_list_layer), dim=-1)\n",
    "                        x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes data of ``n_input`` dimensions into a\n",
    "    latent space of ``n_output`` dimensions.\n",
    "\n",
    "    Uses a fully-connected neural network of ``n_hidden`` layers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input\n",
    "        The dimensionality of the input (data space)\n",
    "    n_output\n",
    "        The dimensionality of the output (latent space)\n",
    "    n_cat_list\n",
    "        A list containing the number of categories\n",
    "        for each category of interest. Each category will be\n",
    "        included using a one-hot encoding\n",
    "    n_layers\n",
    "        The number of fully-connected hidden layers\n",
    "    n_hidden\n",
    "        The number of nodes per hidden layer\n",
    "    dropout_rate\n",
    "        Dropout rate to apply to each of the hidden layers\n",
    "    distribution\n",
    "        Distribution of z\n",
    "    **kwargs\n",
    "        Keyword args for :class:`~scvi.modules._base.FCLayers`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        n_output: int,\n",
    "        connections=None,\n",
    "        n_cat_list: Iterable[int] = None,\n",
    "        n_layers: int = 1,\n",
    "        n_hidden: int = 128,\n",
    "        dropout_rate: float = 0.1,\n",
    "        distribution: str = \"normal\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.distribution = distribution\n",
    "        self.encoder = FCLayersEncoder(\n",
    "            n_in=n_input,\n",
    "            n_out=n_hidden,\n",
    "            connections=connections,\n",
    "            n_cat_list=n_cat_list,\n",
    "            n_layers=n_layers,\n",
    "            n_hidden=n_hidden,\n",
    "            dropout_rate=dropout_rate,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.mean_encoder = nn.Linear(n_hidden, n_output)\n",
    "        self.var_encoder = nn.Linear(n_hidden, n_output)\n",
    "\n",
    "        if distribution == \"ln\":\n",
    "            self.z_transformation = nn.Softmax(dim=-1)\n",
    "        else:\n",
    "            self.z_transformation = identity\n",
    "\n",
    "    def forward(self, x: torch.Tensor, batch_index):\n",
    "        r\"\"\"\n",
    "        The forward computation for a single sample.\n",
    "\n",
    "         #. Encodes the data into latent space using the encoder network\n",
    "         #. Generates a mean \\\\( q_m \\\\) and variance \\\\( q_v \\\\)\n",
    "         #. Samples a new value from an i.i.d. multivariate\n",
    "         normal \\\\( \\\\sim Ne(q_m, \\\\mathbf{I}q_v) \\\\)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x\n",
    "            tensor with shape (n_input,)\n",
    "        batch_index\n",
    "            tensor of batch membership\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        3-tuple of :py:class:`torch.Tensor`\n",
    "            tensors of shape ``(n_latent,)`` for mean and var, and sample\n",
    "\n",
    "        \"\"\"\n",
    "        # Parameters for latent distribution\n",
    "        q = self.encoder(x, batch_index)\n",
    "        q_m = self.mean_encoder(q)\n",
    "        q_v = torch.exp(self.var_encoder(q)) + 1e-4\n",
    "        latent = self.z_transformation(reparameterize_gaussian(q_m, q_v))\n",
    "        return q_m, q_v, latent\n",
    "\n",
    "\n",
    "def log_zinb_positive(\n",
    "    x: torch.Tensor, mu: torch.Tensor,\n",
    "    theta: torch.Tensor, pi: torch.Tensor, eps=1e-8\n",
    "):\n",
    "    \"\"\"\n",
    "    Log likelihood (scalar) of a minibatch according to a zinb model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x\n",
    "        Data\n",
    "    mu\n",
    "        mean of the negative binomial (has to be\n",
    "        positive support) (shape: minibatch x vars)\n",
    "    theta\n",
    "        inverse dispersion parameter (has to be\n",
    "        positive support) (shape: minibatch x vars)\n",
    "    pi\n",
    "        logit of the dropout parameter (real support)\n",
    "        (shape: minibatch x vars)\n",
    "    eps\n",
    "        numerical stability constant\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    We parametrize the bernoulli using the logits,\n",
    "    hence the softplus functions appearing.\n",
    "    \"\"\"\n",
    "    # theta is the dispersion rate. If .ndimension() == 1,\n",
    "    # it is shared for all cells (regardless of batch or labels)\n",
    "    if theta.ndimension() == 1:\n",
    "        theta = theta.view(\n",
    "            1, theta.size(0)\n",
    "        )  # In this case, we reshape theta for broadcasting\n",
    "\n",
    "    softplus_pi = F.softplus(-pi)  # Â uses log(sigmoid(x)) = -softplus(-x)\n",
    "    log_theta_eps = torch.log(theta + eps)\n",
    "    log_theta_mu_eps = torch.log(theta + mu + eps)\n",
    "    pi_theta_log = -pi + theta * (log_theta_eps - log_theta_mu_eps)\n",
    "\n",
    "    case_zero = F.softplus(pi_theta_log) - softplus_pi\n",
    "    mul_case_zero = torch.mul((x < eps).type(torch.float32), case_zero)\n",
    "\n",
    "    case_non_zero = (\n",
    "        -softplus_pi\n",
    "        + pi_theta_log\n",
    "        + x * (torch.log(mu + eps) - log_theta_mu_eps)\n",
    "        + torch.lgamma(x + theta)\n",
    "        - torch.lgamma(theta)\n",
    "        - torch.lgamma(x + 1)\n",
    "    )\n",
    "    mul_case_non_zero = torch.mul((x > eps).type(torch.float32), case_non_zero)\n",
    "\n",
    "    res = mul_case_zero + mul_case_non_zero\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def log_nb_positive(x: torch.Tensor, mu: torch.Tensor, theta: torch.Tensor, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Log likelihood (scalar) of a minibatch according to a nb model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x\n",
    "        data\n",
    "    mu\n",
    "        mean of the negative binomial (has to be positive support) (shape: minibatch x vars)\n",
    "    theta\n",
    "        inverse dispersion parameter (has to be positive support) (shape: minibatch x vars)\n",
    "    eps\n",
    "        numerical stability constant\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    We parametrize the bernoulli using the logits, hence the softplus functions appearing.\n",
    "\n",
    "    \"\"\"\n",
    "    if theta.ndimension() == 1:\n",
    "        theta = theta.view(\n",
    "            1, theta.size(0)\n",
    "        )  # In this case, we reshape theta for broadcasting\n",
    "\n",
    "    log_theta_mu_eps = torch.log(theta + mu + eps)\n",
    "\n",
    "    res = (\n",
    "        theta * (torch.log(theta + eps) - log_theta_mu_eps)\n",
    "        + x * (torch.log(mu + eps) - log_theta_mu_eps)\n",
    "        + torch.lgamma(x + theta)\n",
    "        - torch.lgamma(theta)\n",
    "        - torch.lgamma(x + 1)\n",
    "    )\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def log_mixture_nb(\n",
    "    x: torch.Tensor,\n",
    "    mu_1: torch.Tensor,\n",
    "    mu_2: torch.Tensor,\n",
    "    theta_1: torch.Tensor,\n",
    "    theta_2: torch.Tensor,\n",
    "    pi_logits: torch.Tensor,\n",
    "    eps=1e-8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Log likelihood (scalar) of a minibatch according to a mixture nb model.\n",
    "\n",
    "    pi_logits is the probability (logits) to be in the first component.\n",
    "    For totalVI, the first component should be background.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x\n",
    "        Observed data\n",
    "    mu_1\n",
    "        Mean of the first negative binomial component (has to be positive support) (shape: minibatch x features)\n",
    "    mu_2\n",
    "        Mean of the second negative binomial (has to be positive support) (shape: minibatch x features)\n",
    "    theta_1\n",
    "        First inverse dispersion parameter (has to be positive support) (shape: minibatch x features)\n",
    "    theta_2\n",
    "        Second inverse dispersion parameter (has to be positive support) (shape: minibatch x features)\n",
    "        If None, assume one shared inverse dispersion parameter.\n",
    "    pi_logits\n",
    "        Probability of belonging to mixture component 1 (logits scale)\n",
    "    eps\n",
    "        Numerical stability constant\n",
    "    \"\"\"\n",
    "    if theta_2 is not None:\n",
    "        log_nb_1 = log_nb_positive(x, mu_1, theta_1)\n",
    "        log_nb_2 = log_nb_positive(x, mu_2, theta_2)\n",
    "    # this is intended to reduce repeated computations\n",
    "    else:\n",
    "        theta = theta_1\n",
    "        if theta.ndimension() == 1:\n",
    "            theta = theta.view(\n",
    "                1, theta.size(0)\n",
    "            )  # In this case, we reshape theta for broadcasting\n",
    "\n",
    "        log_theta_mu_1_eps = torch.log(theta + mu_1 + eps)\n",
    "        log_theta_mu_2_eps = torch.log(theta + mu_2 + eps)\n",
    "        lgamma_x_theta = torch.lgamma(x + theta)\n",
    "        lgamma_theta = torch.lgamma(theta)\n",
    "        lgamma_x_plus_1 = torch.lgamma(x + 1)\n",
    "\n",
    "        log_nb_1 = (\n",
    "            theta * (torch.log(theta + eps) - log_theta_mu_1_eps)\n",
    "            + x * (torch.log(mu_1 + eps) - log_theta_mu_1_eps)\n",
    "            + lgamma_x_theta\n",
    "            - lgamma_theta\n",
    "            - lgamma_x_plus_1\n",
    "        )\n",
    "        log_nb_2 = (\n",
    "            theta * (torch.log(theta + eps) - log_theta_mu_2_eps)\n",
    "            + x * (torch.log(mu_2 + eps) - log_theta_mu_2_eps)\n",
    "            + lgamma_x_theta\n",
    "            - lgamma_theta\n",
    "            - lgamma_x_plus_1\n",
    "        )\n",
    "\n",
    "    logsumexp = torch.logsumexp(torch.stack((log_nb_1, log_nb_2 - pi_logits)), dim=0)\n",
    "    softplus_pi = F.softplus(-pi_logits)\n",
    "\n",
    "    log_mixture_nb = logsumexp - softplus_pi\n",
    "\n",
    "    return log_mixture_nb\n",
    "\n",
    "\n",
    "def _convert_mean_disp_to_counts_logits(mu, theta, eps=1e-6):\n",
    "    r\"\"\"\n",
    "    NB parameterizations conversion.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mu\n",
    "        mean of the NB distribution.\n",
    "    theta\n",
    "        inverse overdispersion.\n",
    "    eps\n",
    "        constant used for numerical log stability. (Default value = 1e-6)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    type\n",
    "        the number of failures until the experiment is stopped\n",
    "        and the success probability.\n",
    "    \"\"\"\n",
    "    if not (mu is None) == (theta is None):\n",
    "        raise ValueError(\n",
    "            \"If using the mu/theta NB parameterization, both parameters must be specified\"\n",
    "        )\n",
    "    logits = (mu + eps).log() - (theta + eps).log()\n",
    "    total_count = theta\n",
    "    return total_count, logits\n",
    "\n",
    "\n",
    "def _convert_counts_logits_to_mean_disp(total_count, logits):\n",
    "    \"\"\"\n",
    "    NB parameterizations conversion.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    total_count\n",
    "        Number of failures until the experiment is stopped.\n",
    "    logits\n",
    "        success logits.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    type\n",
    "        the mean and inverse overdispersion of the NB distribution.\n",
    "\n",
    "    \"\"\"\n",
    "    theta = total_count\n",
    "    mu = logits.exp() * theta\n",
    "    return mu, theta\n",
    "\n",
    "\n",
    "def _gamma(theta, mu):\n",
    "    concentration = theta\n",
    "    rate = theta / mu\n",
    "    # Important remark: Gamma is parametrized by the rate = 1/scale!\n",
    "    gamma_d = Gamma(concentration=concentration, rate=rate)\n",
    "    return gamma_d\n",
    "\n",
    "\n",
    "class NegativeBinomial(Distribution):\n",
    "    r\"\"\"\n",
    "    Negative binomial distribution.\n",
    "\n",
    "    One of the following parameterizations must be provided:\n",
    "\n",
    "    (1), (`total_count`, `probs`) where `total_count` is the number of failures until\n",
    "    the experiment is stopped and `probs` the success probability. (2), (`mu`, `theta`)\n",
    "    parameterization, which is the one used by scvi-tools. These parameters respectively\n",
    "    control the mean and inverse dispersion of the distribution.\n",
    "\n",
    "    In the (`mu`, `theta`) parameterization, samples from the negative binomial are generated as follows:\n",
    "\n",
    "    1. :math:`w \\sim \\textrm{Gamma}(\\underbrace{\\theta}_{\\text{shape}}, \\underbrace{\\theta/\\mu}_{\\text{rate}})`\n",
    "    2. :math:`x \\sim \\textrm{Poisson}(w)`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    total_count\n",
    "        Number of failures until the experiment is stopped.\n",
    "    probs\n",
    "        The success probability.\n",
    "    mu\n",
    "        Mean of the distribution.\n",
    "    theta\n",
    "        Inverse dispersion.\n",
    "    validate_args\n",
    "        Raise ValueError if arguments do not match constraints\n",
    "    \"\"\"\n",
    "\n",
    "    arg_constraints = {\n",
    "        \"mu\": constraints.greater_than_eq(0),\n",
    "        \"theta\": constraints.greater_than_eq(0),\n",
    "    }\n",
    "    support = constraints.nonnegative_integer\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_count: Optional[torch.Tensor] = None,\n",
    "        probs: Optional[torch.Tensor] = None,\n",
    "        logits: Optional[torch.Tensor] = None,\n",
    "        mu: Optional[torch.Tensor] = None,\n",
    "        theta: Optional[torch.Tensor] = None,\n",
    "        validate_args: bool = False,\n",
    "    ):\n",
    "        self._eps = 1e-8\n",
    "        if (mu is None) == (total_count is None):\n",
    "            raise ValueError(\n",
    "                \"Please use one of the two possible parameterizations. Refer to the documentation for more information.\"\n",
    "            )\n",
    "\n",
    "        using_param_1 = total_count is not None and (\n",
    "            logits is not None or probs is not None\n",
    "        )\n",
    "        if using_param_1:\n",
    "            logits = logits if logits is not None else probs_to_logits(probs)\n",
    "            total_count = total_count.type_as(logits)\n",
    "            total_count, logits = broadcast_all(total_count, logits)\n",
    "            mu, theta = _convert_counts_logits_to_mean_disp(total_count, logits)\n",
    "        else:\n",
    "            mu, theta = broadcast_all(mu, theta)\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        super().__init__(validate_args=validate_args)\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self.mu\n",
    "\n",
    "    @property\n",
    "    def variance(self):\n",
    "        return self.mean + (self.mean ** 2) / self.theta\n",
    "\n",
    "    def sample(\n",
    "        self, sample_shape: Union[torch.Size, Tuple] = torch.Size()\n",
    "    ) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            gamma_d = self._gamma()\n",
    "            p_means = gamma_d.sample(sample_shape)\n",
    "\n",
    "            # Clamping as distributions objects can have buggy behaviors when\n",
    "            # their parameters are too high\n",
    "            l_train = torch.clamp(p_means, max=1e8)\n",
    "            counts = Poisson(\n",
    "                l_train\n",
    "            ).sample()  # Shape : (n_samples, n_cells_batch, n_vars)\n",
    "            return counts\n",
    "\n",
    "    def log_prob(self, value: torch.Tensor) -> torch.Tensor:\n",
    "        if self._validate_args:\n",
    "            try:\n",
    "                self._validate_sample(value)\n",
    "            except ValueError:\n",
    "                warnings.warn(\n",
    "                    \"The value argument must be within the support of the distribution\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "        return log_nb_positive(value, mu=self.mu, theta=self.theta, eps=self._eps)\n",
    "\n",
    "    def _gamma(self):\n",
    "        return _gamma(self.theta, self.mu)\n",
    "\n",
    "\n",
    "class ZeroInflatedNegativeBinomial(NegativeBinomial):\n",
    "    r\"\"\"\n",
    "    Zero-inflated negative binomial distribution.\n",
    "\n",
    "    One of the following parameterizations must be provided:\n",
    "\n",
    "    (1), (`total_count`, `probs`) where `total_count` is the number of failures until\n",
    "    the experiment is stopped and `probs` the success probability. (2), (`mu`, `theta`)\n",
    "    parameterization, which is the one used by scvi-tools. These parameters respectively\n",
    "    control the mean and inverse dispersion of the distribution.\n",
    "\n",
    "    In the (`mu`, `theta`) parameterization, samples from the negative binomial are generated as follows:\n",
    "\n",
    "    1. :math:`w \\sim \\textrm{Gamma}(\\underbrace{\\theta}_{\\text{shape}}, \\underbrace{\\theta/\\mu}_{\\text{rate}})`\n",
    "    2. :math:`x \\sim \\textrm{Poisson}(w)`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    total_count\n",
    "        Number of failures until the experiment is stopped.\n",
    "    probs\n",
    "        The success probability.\n",
    "    mu\n",
    "        Mean of the distribution.\n",
    "    theta\n",
    "        Inverse dispersion.\n",
    "    zi_logits\n",
    "        Logits scale of zero inflation probability.\n",
    "    validate_args\n",
    "        Raise ValueError if arguments do not match constraints\n",
    "    \"\"\"\n",
    "\n",
    "    arg_constraints = {\n",
    "        \"mu\": constraints.greater_than_eq(0),\n",
    "        \"theta\": constraints.greater_than_eq(0),\n",
    "        \"zi_probs\": constraints.half_open_interval(0.0, 1.0),\n",
    "        \"zi_logits\": constraints.real,\n",
    "    }\n",
    "    support = constraints.nonnegative_integer\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        total_count: Optional[torch.Tensor] = None,\n",
    "        probs: Optional[torch.Tensor] = None,\n",
    "        logits: Optional[torch.Tensor] = None,\n",
    "        mu: Optional[torch.Tensor] = None,\n",
    "        theta: Optional[torch.Tensor] = None,\n",
    "        zi_logits: Optional[torch.Tensor] = None,\n",
    "        validate_args: bool = False,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            total_count=total_count,\n",
    "            probs=probs,\n",
    "            logits=logits,\n",
    "            mu=mu,\n",
    "            theta=theta,\n",
    "            validate_args=validate_args,\n",
    "        )\n",
    "        self.zi_logits, self.mu, self.theta = broadcast_all(\n",
    "            zi_logits, self.mu, self.theta\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        pi = self.zi_probs\n",
    "        return (1 - pi) * self.mu\n",
    "\n",
    "    @property\n",
    "    def variance(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @lazy_property\n",
    "    def zi_logits(self) -> torch.Tensor:\n",
    "        return probs_to_logits(self.zi_probs, is_binary=True)\n",
    "\n",
    "    @lazy_property\n",
    "    def zi_probs(self) -> torch.Tensor:\n",
    "        return logits_to_probs(self.zi_logits, is_binary=True)\n",
    "\n",
    "    def sample(\n",
    "        self, sample_shape: Union[torch.Size, Tuple] = torch.Size()\n",
    "    ) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            samp = super().sample(sample_shape=sample_shape)\n",
    "            is_zero = torch.rand_like(samp) <= self.zi_probs\n",
    "            samp[is_zero] = 0.0\n",
    "            return samp\n",
    "\n",
    "    def log_prob(self, value: torch.Tensor) -> torch.Tensor:\n",
    "        try:\n",
    "            self._validate_sample(value)\n",
    "        except ValueError:\n",
    "            warnings.warn(\n",
    "                \"The value argument must be within the support of the distribution\",\n",
    "                UserWarning,\n",
    "            )\n",
    "        return log_zinb_positive(value, self.mu, self.theta, self.zi_logits, eps=1e-08)\n",
    "\n",
    "\n",
    "class NegativeBinomialMixture(Distribution):\n",
    "    \"\"\"\n",
    "    Negative binomial mixture distribution.\n",
    "\n",
    "    See :class:`~scvi.distributions.NegativeBinomial` for further description\n",
    "    of parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mu1\n",
    "        Mean of the component 1 distribution.\n",
    "    mu2\n",
    "        Mean of the component 2 distribution.\n",
    "    theta1\n",
    "        Inverse dispersion for component 1.\n",
    "    mixture_logits\n",
    "        Logits scale probability of belonging to component 1.\n",
    "    theta2\n",
    "        Inverse dispersion for component 1. If `None`, assumed to be equal to `theta1`.\n",
    "    validate_args\n",
    "        Raise ValueError if arguments do not match constraints\n",
    "    \"\"\"\n",
    "\n",
    "    arg_constraints = {\n",
    "        \"mu1\": constraints.greater_than_eq(0),\n",
    "        \"mu2\": constraints.greater_than_eq(0),\n",
    "        \"theta1\": constraints.greater_than_eq(0),\n",
    "        \"mixture_probs\": constraints.half_open_interval(0.0, 1.0),\n",
    "        \"mixture_logits\": constraints.real,\n",
    "    }\n",
    "    support = constraints.nonnegative_integer\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mu1: torch.Tensor,\n",
    "        mu2: torch.Tensor,\n",
    "        theta1: torch.Tensor,\n",
    "        mixture_logits: torch.Tensor,\n",
    "        theta2: Optional[torch.Tensor] = None,\n",
    "        validate_args: bool = False,\n",
    "    ):\n",
    "\n",
    "        (\n",
    "            self.mu1,\n",
    "            self.theta1,\n",
    "            self.mu2,\n",
    "            self.mixture_logits,\n",
    "        ) = broadcast_all(mu1, theta1, mu2, mixture_logits)\n",
    "\n",
    "        super().__init__(validate_args=validate_args)\n",
    "\n",
    "        if theta2 is not None:\n",
    "            self.theta2 = broadcast_all(mu1, theta2)\n",
    "        else:\n",
    "            self.theta2 = None\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        pi = self.mixture_probs\n",
    "        return pi * self.mu1 + (1 - pi) * self.mu2\n",
    "\n",
    "    @lazy_property\n",
    "    def mixture_probs(self) -> torch.Tensor:\n",
    "        return logits_to_probs(self.mixture_logits, is_binary=True)\n",
    "\n",
    "    def sample(\n",
    "        self, sample_shape: Union[torch.Size, Tuple] = torch.Size()\n",
    "    ) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            pi = self.mixture_probs\n",
    "            mixing_sample = torch.distributions.Bernoulli(pi).sample()\n",
    "            mu = self.mu1 * mixing_sample + self.mu2 * (1 - mixing_sample)\n",
    "            if self.theta2 is None:\n",
    "                theta = self.theta1\n",
    "            else:\n",
    "                theta = self.theta1 * mixing_sample + self.theta2 * (1 - mixing_sample)\n",
    "            gamma_d = _gamma(mu, theta)\n",
    "            p_means = gamma_d.sample(sample_shape)\n",
    "\n",
    "            # Clamping as distributions objects can have buggy behaviors when\n",
    "            # their parameters are too high\n",
    "            l_train = torch.clamp(p_means, max=1e8)\n",
    "            counts = Poisson(\n",
    "                l_train\n",
    "            ).sample()  # Shape : (n_samples, n_cells_batch, n_features)\n",
    "            return counts\n",
    "\n",
    "    def log_prob(self, value: torch.Tensor) -> torch.Tensor:\n",
    "        try:\n",
    "            self._validate_sample(value)\n",
    "        except ValueError:\n",
    "            warnings.warn(\n",
    "                \"The value argument must be within the support of the distribution\",\n",
    "                UserWarning,\n",
    "            )\n",
    "        return log_mixture_nb(\n",
    "            value,\n",
    "            self.mu1,\n",
    "            self.mu2,\n",
    "            self.theta1,\n",
    "            self.theta2,\n",
    "            self.mixture_logits,\n",
    "            eps=1e-08,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "072d4f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Normal, Poisson\n",
    "from torch.distributions import kl_divergence as kl\n",
    "from typing import Dict, Tuple\n",
    "from typing import Iterable, Optional\n",
    "#from utils import Encoder, FCLayers\n",
    "#from utils import NegativeBinomial, ZeroInflatedNegativeBinomial\n",
    "\n",
    "\n",
    "try:\n",
    "    from typing import Literal\n",
    "except ImportError:\n",
    "    try:\n",
    "        from typing_extensions import Literal\n",
    "    except ImportError:\n",
    "\n",
    "        class LiteralMeta(type):\n",
    "            def __getitem__(cls, values):\n",
    "                if not isinstance(values, tuple):\n",
    "                    values = (values,)\n",
    "                return type(\"Literal_\", (Literal,), dict(__args__=values))\n",
    "\n",
    "        class Literal(metaclass=LiteralMeta):\n",
    "            pass\n",
    "\n",
    "\n",
    "class DecoderSCVI(nn.Module):\n",
    "    \"\"\"\n",
    "    Decodes data from latent space of ``n_input``\n",
    "    dimensions ``n_output``dimensions.\n",
    "\n",
    "    Uses a fully-connected neural network of ``n_hidden`` layers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input\n",
    "        The dimensionality of the input (latent space)\n",
    "    n_output\n",
    "        The dimensionality of the output (data space)\n",
    "    n_cat_list\n",
    "        A list containing the number of categories\n",
    "        for each category of interest. Each category will be\n",
    "        included using a one-hot encoding\n",
    "    n_layers\n",
    "        The number of fully-connected hidden layers\n",
    "    n_hidden\n",
    "        The number of nodes per hidden layer\n",
    "    dropout_rate\n",
    "        Dropout rate to apply to each of the hidden layers\n",
    "    inject_covariates\n",
    "        Whether to inject covariates in each layer,\n",
    "        or just the first (default).\n",
    "    use_batch_norm\n",
    "        Whether to use batch norm in layers\n",
    "    use_layer_norm\n",
    "        Whether to use layer norm in layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        n_output: int,\n",
    "        n_cat_list: Iterable[int] = None,\n",
    "        n_layers: int = 1,\n",
    "        n_hidden: int = 128,\n",
    "        inject_covariates: bool = True,\n",
    "        use_batch_norm: bool = False,\n",
    "        use_layer_norm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.px_decoder = FCLayers(\n",
    "            n_in=n_input,\n",
    "            n_out=n_hidden,\n",
    "            n_cat_list=n_cat_list,\n",
    "            n_layers=n_layers,\n",
    "            n_hidden=n_hidden,\n",
    "            dropout_rate=0,\n",
    "            inject_covariates=inject_covariates,\n",
    "            use_batch_norm=use_batch_norm,\n",
    "            use_layer_norm=use_layer_norm,\n",
    "        )\n",
    "\n",
    "        self.n_cat_list = n_cat_list\n",
    "\n",
    "        # mean gamma\n",
    "        self.px_scale_decoder = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_output),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "        # dispersion: here we only deal with gene-cell dispersion case\n",
    "        self.px_r_decoder = nn.Linear(n_hidden, n_output)\n",
    "\n",
    "        # dropout\n",
    "        self.px_dropout_decoder = nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(\n",
    "        self, dispersion: str, z: torch.Tensor,\n",
    "        library: torch.Tensor, batch_index\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The forward computation for a single sample.\n",
    "\n",
    "         #. Decodes the data from the latent space using the decoder network\n",
    "         #. Returns parameters for the ZINB distribution of expression\n",
    "         #. If ``dispersion != 'gene-cell'`` then value\n",
    "         for that param will be ``None``\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dispersion\n",
    "            One of the following\n",
    "\n",
    "            * ``'gene'`` - dispersion parameter of NB is\n",
    "            constant per gene across cells\n",
    "            * ``'gene-batch'`` - dispersion can differ\n",
    "            between different batches\n",
    "            * ``'gene-label'`` - dispersion can differ\n",
    "            between different labels\n",
    "            * ``'gene-cell'`` - dispersion can differ\n",
    "            for every gene in every cell\n",
    "        z :\n",
    "            tensor with shape ``(n_input,)``\n",
    "        library\n",
    "            library size\n",
    "        cat_list\n",
    "            list of category membership(s) for this sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        4-tuple of :py:class:`torch.Tensor`\n",
    "            parameters for the ZINB distribution of expression\n",
    "\n",
    "        \"\"\"\n",
    "        # The decoder returns values for the parameters\n",
    "        # of the ZINB distribution\n",
    "        px = self.px_decoder(z, batch_index)\n",
    "        px_scale = self.px_scale_decoder(px)\n",
    "        px_dropout = self.px_dropout_decoder(px)\n",
    "        # Clamp to high value: exp(12) ~ 160000 to\n",
    "        # avoid nans (computational stability)\n",
    "        px_rate = torch.exp(library) * px_scale  # torch.clamp( , max=12)\n",
    "        px_r = self.px_r_decoder(px) if dispersion == \"gene-cell\" else None\n",
    "        return px_scale, px_r, px_rate, px_dropout, px\n",
    "\n",
    "\n",
    "def one_hot(index, n_cat):\n",
    "    onehot = torch.zeros(index.size(0), n_cat, device=index.device)\n",
    "    onehot.scatter_(1, index.type(torch.long), 1)\n",
    "    return onehot.type(torch.float32)\n",
    "\n",
    "\n",
    "def reparameterize_gaussian(mu, var):\n",
    "    return Normal(mu, var.sqrt()).rsample()\n",
    "\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def loss_function(\n",
    "        qz_m, qz_v, x, px_rate, px_r, px_dropout,\n",
    "        ql_m, ql_v, use_observed_lib_size,\n",
    "        local_l_mean, local_l_var):\n",
    "    mean = torch.zeros_like(qz_m)\n",
    "    scale = torch.ones_like(qz_v)\n",
    "    kl_divergence_z = kl(\n",
    "        Normal(qz_m, torch.sqrt(qz_v)), Normal(mean, scale)).sum(\n",
    "            dim=1)\n",
    "    if not use_observed_lib_size:\n",
    "        kl_divergence_l = kl(\n",
    "            Normal(ql_m, torch.sqrt(ql_v)),\n",
    "            Normal(local_l_mean, torch.sqrt(local_l_var)),\n",
    "        ).sum(dim=1)\n",
    "    else:\n",
    "        kl_divergence_l = 0.0\n",
    "    kl_divergence = kl_divergence_z\n",
    "    reconst_loss = get_reconstruction_loss(\n",
    "        x, px_rate, px_r, px_dropout)\n",
    "    return reconst_loss + kl_divergence_l, kl_divergence\n",
    "\n",
    "\n",
    "def get_reconstruction_loss(\n",
    "        x, px_rate, px_r, px_dropout, gene_likelihood=\"zinb\"):\n",
    "    # Reconstruction Loss\n",
    "    if gene_likelihood == \"zinb\":\n",
    "        reconst_loss = (\n",
    "            -ZeroInflatedNegativeBinomial(\n",
    "                mu=px_rate, theta=px_r, zi_logits=px_dropout\n",
    "            )\n",
    "            .log_prob(x)\n",
    "            .sum(dim=-1)\n",
    "        )\n",
    "    elif gene_likelihood == \"nb\":\n",
    "        reconst_loss = (\n",
    "            -NegativeBinomial(\n",
    "                mu=px_rate, theta=px_r).log_prob(x).sum(dim=-1)\n",
    "        )\n",
    "    elif gene_likelihood == \"poisson\":\n",
    "        reconst_loss = -Poisson(px_rate).log_prob(x).sum(dim=-1)\n",
    "    return reconst_loss\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        connections=None,\n",
    "        n_celltypes: int = 10,\n",
    "        n_batch: int = 0,\n",
    "        n_labels: int = 0,\n",
    "        n_hidden: int = 128,\n",
    "        n_latent: int = 10,\n",
    "        n_layers: int = 1,\n",
    "        n_continuous_cov: int = 0,\n",
    "        n_cats_per_cov: Optional[Iterable[int]] = None,\n",
    "        dropout_rate: float = 0.1,\n",
    "        dispersion: str = \"gene\",\n",
    "        log_variational: bool = True,\n",
    "        gene_likelihood: str = \"zinb\",\n",
    "        latent_distribution: str = \"normal\",\n",
    "        encode_covariates: bool = False,\n",
    "        deeply_inject_covariates: bool = True,\n",
    "        use_batch_norm: Literal[\"encoder\", \"decoder\", \"none\", \"both\"] = \"both\",\n",
    "        use_layer_norm: Literal[\"encoder\", \"decoder\", \"none\", \"both\"] = \"none\",\n",
    "        use_observed_lib_size: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_celltypes = n_celltypes\n",
    "        self.connections = connections\n",
    "        self.dispersion = dispersion\n",
    "        if n_batch > 0:\n",
    "            print(\n",
    "                \"Setting dispersion to gene-batch and\"\n",
    "                \" encode_covariates to True\")\n",
    "            self.dispersion = \"gene-batch\"\n",
    "            encode_covariates = True\n",
    "        self.n_latent = n_latent\n",
    "        self.log_variational = log_variational\n",
    "        self.gene_likelihood = gene_likelihood\n",
    "        # Automatically deactivate if useless\n",
    "        self.n_batch = n_batch\n",
    "        self.n_labels = n_labels\n",
    "        self.latent_distribution = latent_distribution\n",
    "        self.encode_covariates = encode_covariates\n",
    "        self.use_observed_lib_size = use_observed_lib_size\n",
    "\n",
    "        if self.dispersion == \"gene\":\n",
    "            self.px_r = torch.nn.Parameter(torch.randn(n_input))\n",
    "        elif self.dispersion == \"gene-batch\":\n",
    "            self.px_r = torch.nn.Parameter(torch.randn(n_input, n_batch))\n",
    "        elif self.dispersion == \"gene-label\":\n",
    "            self.px_r = torch.nn.Parameter(torch.randn(n_input, n_labels))\n",
    "        elif self.dispersion == \"gene-cell\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"dispersion must be one of ['gene', 'gene-batch',\"\n",
    "                \" 'gene-label', 'gene-cell'], but input was \"\n",
    "                \"{}.format(self.dispersion)\"\n",
    "            )\n",
    "\n",
    "        use_batch_norm_encoder = use_batch_norm == \"encoder\" or\\\n",
    "            use_batch_norm == \"both\"\n",
    "        use_batch_norm_decoder = use_batch_norm == \"decoder\" or\\\n",
    "            use_batch_norm == \"both\"\n",
    "        use_layer_norm_encoder = use_layer_norm == \"encoder\" or\\\n",
    "            use_layer_norm == \"both\"\n",
    "        use_layer_norm_decoder = use_layer_norm == \"decoder\" or\\\n",
    "            use_layer_norm == \"both\"\n",
    "\n",
    "        # z encoder goes from the n_input-dimensional data to an n_latent-d\n",
    "        # latent space representation\n",
    "        n_input_encoder = n_input + n_continuous_cov * encode_covariates\n",
    "        cat_list = [n_batch] + list(\n",
    "            [] if n_cats_per_cov is None else n_cats_per_cov)\n",
    "        self.cat_list = cat_list\n",
    "        encoder_cat_list = cat_list if encode_covariates else None\n",
    "        self.encoder_cat_list = encoder_cat_list\n",
    "        self.z_encoder = Encoder(\n",
    "            n_input_encoder,\n",
    "            n_latent,\n",
    "            self.connections,\n",
    "            n_cat_list=encoder_cat_list,\n",
    "            n_layers=n_layers,\n",
    "            n_hidden=n_hidden,\n",
    "            dropout_rate=dropout_rate,\n",
    "            distribution=latent_distribution,\n",
    "            inject_covariates=deeply_inject_covariates,\n",
    "            use_batch_norm=use_batch_norm_encoder,\n",
    "            use_layer_norm=use_layer_norm_encoder,\n",
    "        )\n",
    "\n",
    "        self.ctpred_linear = nn.Linear(n_latent, n_celltypes)\n",
    "        self.ctpred_activation = nn.ReLU()\n",
    "\n",
    "        # l encoder goes from n_input-dimensional data to 1-d library size\n",
    "        self.l_encoder = Encoder(\n",
    "            n_input_encoder,\n",
    "            1,\n",
    "            None,\n",
    "            n_layers=1,\n",
    "            n_cat_list=encoder_cat_list,\n",
    "            n_hidden=n_hidden,\n",
    "            dropout_rate=dropout_rate,\n",
    "            inject_covariates=deeply_inject_covariates,\n",
    "            use_batch_norm=use_batch_norm_encoder,\n",
    "            use_layer_norm=use_layer_norm_encoder,\n",
    "        )\n",
    "        # decoder goes from n_latent-dimensional space to n_input-d data\n",
    "        n_input_decoder = n_latent + n_continuous_cov\n",
    "        self.decoder = DecoderSCVI(\n",
    "            n_input_decoder,\n",
    "            n_input,\n",
    "            n_cat_list=cat_list,\n",
    "            n_layers=n_layers,\n",
    "            n_hidden=n_hidden,\n",
    "            inject_covariates=deeply_inject_covariates,\n",
    "            use_batch_norm=use_batch_norm_decoder,\n",
    "            use_layer_norm=use_layer_norm_decoder,\n",
    "        )\n",
    "\n",
    "    def get_latents(self, x, y=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the result of ``sample_from_posterior_z`` inside a list.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x\n",
    "            tensor of values with shape ``(batch_size, inputsize)``\n",
    "        y\n",
    "            tensor of cell-types labels with shape\n",
    "            ``(batch_size, n_labels)`` (Default value = None)\n",
    "        Returns\n",
    "        -------\n",
    "        type\n",
    "            one element list of tensor\n",
    "        \"\"\"\n",
    "        return [self.sample_from_posterior_z(x, y)]\n",
    "\n",
    "    def sample_from_posterior_z(\n",
    "        self, x, batch_index=None, y=None, give_mean=False, n_samples=5000\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Samples the tensor of latent values from the posterior.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x\n",
    "            tensor of values with shape ``(batch_size, inputsize)``\n",
    "        y\n",
    "            tensor of cell-types labels with shape ``(batch_size, n_labels)``\n",
    "            (Default value = None)\n",
    "        give_mean\n",
    "            is True when we want the mean of the posterior\n",
    "            distribution rather than sampling (Default value = False)\n",
    "        n_samples\n",
    "            how many MC samples to average over for\n",
    "            transformed mean (Default value = 5000)\n",
    "        Returns\n",
    "        -------\n",
    "        type\n",
    "            tensor of shape ``(batch_size, lvsize)``\n",
    "        \"\"\"\n",
    "        if self.log_variational:\n",
    "            x = torch.log(1 + x)\n",
    "        qz_m, qz_v, z = self.z_encoder(\n",
    "            x, batch_index)  # y only used in VAEC\n",
    "\n",
    "        if give_mean:\n",
    "            if self.latent_distribution == \"ln\":\n",
    "                samples = Normal(qz_m, qz_v.sqrt()).sample([n_samples])\n",
    "                z = self.z_encoder.z_transformation(samples, batch_index)\n",
    "                z = z.mean(dim=0)\n",
    "            else:\n",
    "                z = qz_m\n",
    "        return z\n",
    "\n",
    "    def sample_from_posterior_l(\n",
    "        self, x, batch_index=None, give_mean=True\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Samples the tensor of library sizes from the posterior.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x\n",
    "            tensor of values with shape ``(batch_size, inputsize)``\n",
    "        y\n",
    "            tensor of cell-types labels with shape ``(batch_size, n_labels)``\n",
    "        give_mean\n",
    "            Return mean or sample\n",
    "        Returns\n",
    "        -------\n",
    "        type\n",
    "            tensor of shape ``(batch_size, 1)``\n",
    "        \"\"\"\n",
    "        if self.log_variational:\n",
    "            x = torch.log(1 + x)\n",
    "        ql_m, ql_v, library = self.l_encoder(x, batch_index)\n",
    "        if give_mean is False:\n",
    "            library = library\n",
    "        else:\n",
    "            library = torch.distributions.LogNormal(ql_m, ql_v.sqrt()).mean\n",
    "        return library\n",
    "\n",
    "    def get_sample_scale(\n",
    "        self, x, batch_index=None, y=None, n_samples=1, transform_batch=None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the tensor of predicted frequencies of expression.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x\n",
    "            tensor of values with shape ``(batch_size, inputsize)``\n",
    "        batch_index\n",
    "            array that indicates which batch the cells\n",
    "            belong to with shape ``batch_size`` (Default value = None)\n",
    "        y\n",
    "            tensor of cell-types labels with shape ``(batch_size,\n",
    "            n_labels)`` (Default value = None)\n",
    "        n_samples\n",
    "            number of samples (Default value = 1)\n",
    "        transform_batch\n",
    "            int of batch to transform samples into (Default value = None)\n",
    "        Returns\n",
    "        -------\n",
    "        type\n",
    "            tensor of predicted frequencies of expression\n",
    "            with shape ``(batch_size, inputsize)``\n",
    "        \"\"\"\n",
    "        return self.inference(\n",
    "            x,\n",
    "            batch_index=batch_index,\n",
    "            y=y,\n",
    "            n_samples=n_samples,\n",
    "            transform_batch=transform_batch,\n",
    "        )[\"px_scale\"]\n",
    "\n",
    "    def get_sample_rate(\n",
    "        self, x, batch_index=None, y=None, n_samples=1, transform_batch=None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the tensor of means of the negative binomial distribution.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x\n",
    "            tensor of values with shape ``(batch_size, inputsize)``\n",
    "        y\n",
    "            tensor of cell-types labels with shape\n",
    "            ``(batch_size, n_labels)`` (Default value = None)\n",
    "        batch_index\n",
    "            array that indicates which batch the cells belong to with\n",
    "            shape ``batch_size`` (Default value = None)\n",
    "        n_samples\n",
    "            number of samples (Default value = 1)\n",
    "        transform_batch\n",
    "            int of batch to transform samples into (Default value = None)\n",
    "        Returns\n",
    "        -------\n",
    "        type\n",
    "            tensor of means of the negative binomial distribution with\n",
    "            shape ``(batch_size, inputsize)``\n",
    "        \"\"\"\n",
    "        return self.inference(\n",
    "            x,\n",
    "            batch_index=batch_index,\n",
    "            y=y,\n",
    "            n_samples=n_samples,\n",
    "            transform_batch=transform_batch,\n",
    "        )[\"px_rate\"]\n",
    "\n",
    "    def inference(\n",
    "        self, x, batch_index=None, y=None, n_samples=1, transform_batch=None\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Helper function used in forward pass.\"\"\"\n",
    "        x_ = x\n",
    "        if self.use_observed_lib_size:\n",
    "            library = torch.log(x.sum(1)).unsqueeze(1)\n",
    "        if self.log_variational:\n",
    "            x_ = torch.log(1 + x_)\n",
    "\n",
    "        # Sampling\n",
    "        qz_m, qz_v, z = self.z_encoder(x_, batch_index)\n",
    "        ql_m, ql_v, library_encoded = self.l_encoder(x_, batch_index)\n",
    "        if not self.use_observed_lib_size:\n",
    "            library = library_encoded\n",
    "\n",
    "        # Predict celltypes using z\n",
    "        ctpred = self.ctpred_activation(\n",
    "            self.ctpred_linear(qz_m))\n",
    "\n",
    "        if n_samples > 1:\n",
    "            qz_m = qz_m.unsqueeze(\n",
    "                0).expand((n_samples, qz_m.size(0), qz_m.size(1)))\n",
    "            qz_v = qz_v.unsqueeze(\n",
    "                0).expand((n_samples, qz_v.size(0), qz_v.size(1)))\n",
    "            # when z is normal, untran_z == z\n",
    "            untran_z = Normal(qz_m, qz_v.sqrt()).sample()\n",
    "            z = self.z_encoder.z_transformation(untran_z)\n",
    "            ql_m = ql_m.unsqueeze(\n",
    "                0).expand((n_samples, ql_m.size(0), ql_m.size(1)))\n",
    "            ql_v = ql_v.unsqueeze(\n",
    "                0).expand((n_samples, ql_v.size(0), ql_v.size(1)))\n",
    "            if self.use_observed_lib_size:\n",
    "                library = library.unsqueeze(0).expand(\n",
    "                    (n_samples, library.size(0), library.size(1))\n",
    "                )\n",
    "            else:\n",
    "                library = Normal(ql_m, ql_v.sqrt()).sample()\n",
    "\n",
    "        if transform_batch is not None:\n",
    "            dec_batch_index = transform_batch * torch.ones_like(batch_index)\n",
    "        else:\n",
    "            dec_batch_index = batch_index\n",
    "\n",
    "        px_scale, px_r, px_rate, px_dropout, px = self.decoder(\n",
    "            self.dispersion, z, library, batch_index\n",
    "        )\n",
    "        if self.dispersion == \"gene-label\":\n",
    "            px_r = F.linear(\n",
    "                one_hot(y, self.n_labels), self.px_r\n",
    "            )  # px_r gets transposed - last dimension is nb genes\n",
    "        elif self.dispersion == \"gene-batch\":\n",
    "            px_r = F.linear(one_hot(dec_batch_index, self.n_batch), self.px_r)\n",
    "        elif self.dispersion == \"gene\":\n",
    "            px_r = self.px_r\n",
    "        px_r = torch.exp(px_r)\n",
    "\n",
    "        return dict(\n",
    "            px_scale=px_scale,\n",
    "            px_r=px_r,\n",
    "            px_rate=px_rate,\n",
    "            px_dropout=px_dropout,\n",
    "            qz_m=qz_m,\n",
    "            qz_v=qz_v,\n",
    "            z=z,\n",
    "            ql_m=ql_m,\n",
    "            ql_v=ql_v,\n",
    "            library=library,\n",
    "            px=px,\n",
    "            ctpred=ctpred\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, x, batch_index=None, y=None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Parameters for z latent distribution\n",
    "        outputs = self.inference(x, batch_index, y)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd897f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/Users/brendamelano/opt/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/warnings.py:53: LightningDeprecationWarning: pytorch_lightning.utilities.warnings.rank_zero_deprecation has been deprecated in v1.6 and will be removed in v1.8. Use the equivalent function from the pytorch_lightning.utilities.rank_zero module instead.\n",
      "  new_rank_zero_deprecation(\n",
      "/Users/brendamelano/opt/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/warnings.py:58: LightningDeprecationWarning: The `pytorch_lightning.loggers.base.rank_zero_experiment` is deprecated in v1.7 and will be removed in v1.9. Please use `pytorch_lightning.loggers.logger.rank_zero_experiment` instead.\n",
      "  return new_rank_zero_deprecation(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import scanpy as sc\n",
    "import scvelo as scv\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import scipy.stats\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import scipy.sparse as sp_sparse\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the VAE model class\n",
    "class VAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input,\n",
    "        connections=None,\n",
    "        n_celltypes=0,\n",
    "        n_batch=0,\n",
    "        n_labels=0,\n",
    "        n_hidden=128,\n",
    "        n_latent=10,\n",
    "        n_layers=1,\n",
    "        dropout_rate=0.1,\n",
    "        predict_celltype=False\n",
    "    ):\n",
    "        super(VAE, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.n_latent = n_latent\n",
    "        self.predict_celltype = predict_celltype\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(n_hidden, n_latent * 2)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(n_latent, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(n_hidden, n_input)\n",
    "        )\n",
    "\n",
    "        # Optional classifier for cell type prediction\n",
    "        if self.predict_celltype:\n",
    "            self.classifier = nn.Linear(n_latent, n_celltypes)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = h.chunk(2, dim=-1)\n",
    "        # Clamp logvar to prevent numerical issues\n",
    "        logvar = torch.clamp(logvar, min=-10, max=10)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        output = {\n",
    "            'recon_x': recon_x,\n",
    "            'mu': mu,\n",
    "            'logvar': logvar\n",
    "        }\n",
    "        if self.predict_celltype:\n",
    "            output['state_pred'] = self.classifier(z)\n",
    "        return output\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(recon_x, x, mu, logvar, state_pred=None, state_true=None, loss_scalers=[1000, 1, 1]):\n",
    "    # Reconstruction loss\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "\n",
    "    # KL divergence\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    # Total loss\n",
    "    loss = (recon_loss / loss_scalers[0]) + (kld / loss_scalers[1])\n",
    "\n",
    "    # Cell type prediction loss\n",
    "    if state_pred is not None and state_true is not None:\n",
    "        ce_loss = F.cross_entropy(state_pred, state_true)\n",
    "        loss += ce_loss / loss_scalers[2]\n",
    "    else:\n",
    "        ce_loss = torch.tensor(0.0).to(device)\n",
    "\n",
    "    return loss, recon_loss, kld, ce_loss\n",
    "\n",
    "def main(\n",
    "    gmtpath,\n",
    "    nparpaths,\n",
    "    outdir,\n",
    "    numlvs,\n",
    "    metapaths,\n",
    "    dont_train,\n",
    "    genepath,\n",
    "    existingmodelpath,\n",
    "    use_connections,\n",
    "    loss_scalers,\n",
    "    predict_celltypes,\n",
    "    num_celltypes,\n",
    "    filter_var,\n",
    "    num_genes,\n",
    "    include_batches\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function to load data, initialize the model, train it, and return the trained model along with necessary variables.\n",
    "    \"\"\"\n",
    "    # Load the GMT matrix and gene list\n",
    "    gmtmat, tfs, genes_gmtmat = make_gmtmat(gmtpath, outdir, genepath)\n",
    "\n",
    "    # Load input data\n",
    "    dict_inputs = load_inputs(\n",
    "        nparpaths, gmtmat, outdir, genes_gmtmat, metapaths\n",
    "    )\n",
    "    expar = dict_inputs[\"expar\"]          # Expression matrix (cells x genes)\n",
    "    metadf = dict_inputs[\"metadf\"]        # Metadata DataFrame\n",
    "    barcodes = dict_inputs[\"barcodes\"]    # Cell barcodes\n",
    "    genes_expar = dict_inputs[\"genes\"]    # Genes from expression data\n",
    "    gmtmat_original = dict_inputs[\"gmtmat\"]  # Original gmtmat (genes x tfs)\n",
    "\n",
    "    # Align gmtmat to match the genes in expar\n",
    "    gmtmat_df = pd.DataFrame(gmtmat_original, index=genes_gmtmat, columns=tfs)\n",
    "    gmtmat_aligned_df = gmtmat_df.reindex(genes_expar).fillna(0)\n",
    "    gmtmat_aligned = gmtmat_aligned_df.values  # Shape: (n_genes, n_tfs)\n",
    "\n",
    "    # Prepare state labels if predicting cell types\n",
    "    if predict_celltypes:\n",
    "        # Assuming 'State' is a column in metadf\n",
    "        states = metadf['State'].unique().tolist()\n",
    "        num_states = len(states)\n",
    "        metadf['State_Code'] = metadf['State'].astype('category').cat.codes\n",
    "        state_labels = torch.from_numpy(metadf['State_Code'].values).long().to(device)\n",
    "    else:\n",
    "        states = []\n",
    "        state_labels = None\n",
    "        num_states = 0  # Set to 0 if not predicting states\n",
    "\n",
    "    # Handle batch indices if necessary\n",
    "    if include_batches:\n",
    "        if 'Batch' in metadf.columns:\n",
    "            metadf['Batch_Code'] = metadf['Batch'].astype('category').cat.codes\n",
    "            batch_idxs = metadf['Batch_Code'].values\n",
    "        else:\n",
    "            batch_idxs = None\n",
    "    else:\n",
    "        batch_idxs = None\n",
    "\n",
    "    # Convert expression data to PyTorch tensor\n",
    "    expar = torch.from_numpy(expar).float().to(device)\n",
    "\n",
    "    # Check for NaNs or Infs in expar\n",
    "    if torch.isnan(expar).any() or torch.isinf(expar).any():\n",
    "        print(\"NaN or Inf detected in expar. Replacing with zeros.\")\n",
    "        expar = torch.where(torch.isnan(expar), torch.zeros_like(expar), expar)\n",
    "        expar = torch.where(torch.isinf(expar), torch.zeros_like(expar), expar)\n",
    "\n",
    "    # Initialize the model\n",
    "    vae = VAE(\n",
    "        n_input=expar.shape[1],  # Number of genes\n",
    "        connections=gmtmat_aligned if use_connections else None,\n",
    "        n_celltypes=num_states,\n",
    "        n_batch=0 if batch_idxs is None else len(np.unique(batch_idxs)),\n",
    "        n_labels=0,  # Adjust if you have labels\n",
    "        n_hidden=gmtmat_aligned.shape[1] if use_connections else 128,\n",
    "        n_latent=numlvs,\n",
    "        n_layers=1,\n",
    "        dropout_rate=0.1,\n",
    "        predict_celltype=predict_celltypes\n",
    "    ).to(device)\n",
    "\n",
    "    # Initialize the optimizer with a lower learning rate\n",
    "    optimizer = torch.optim.Adam(vae.parameters(), lr=1e-4)\n",
    "\n",
    "    # Paths for saving the model and checkpoints\n",
    "    modelpath = os.path.join(outdir, \"vae.pt\")\n",
    "    chkpath = os.path.join(outdir, \"vae_chkp.pt\")\n",
    "    logdir = outdir\n",
    "\n",
    "    # Load existing model if specified\n",
    "    if existingmodelpath != 'NA':\n",
    "        vae, optimizer = load_existing_model(existingmodelpath, chkpath, vae, optimizer)\n",
    "\n",
    "    # Training parameters\n",
    "    MINIBATCH = 64  # Adjust based on your hardware\n",
    "    MAXEPOCH = 50   # Set the number of epochs for training\n",
    "    loss_scalers = loss_scalers  # As provided\n",
    "\n",
    "    # Train the model\n",
    "    if not dont_train:\n",
    "        vae = train_model(\n",
    "            vae, optimizer, MINIBATCH, MAXEPOCH,\n",
    "            expar, logdir,\n",
    "            modelpath, chkpath, state_labels,\n",
    "            loss_scalers, predict_celltypes,\n",
    "            states, batch_idxs\n",
    "        )\n",
    "\n",
    "    # Return the trained model and other necessary variables\n",
    "    return vae, gmtmat_aligned, tfs, states\n",
    "\n",
    "def train_model(\n",
    "    vae,\n",
    "    optimizer,\n",
    "    MINIBATCH,\n",
    "    MAXEPOCH,\n",
    "    expar,\n",
    "    logdir,\n",
    "    modelpath,\n",
    "    chkpath,\n",
    "    state_labels,\n",
    "    loss_scalers,\n",
    "    predict_celltypes,\n",
    "    states=[],\n",
    "    batch_idxs=None\n",
    "):\n",
    "    criterion_class = torch.nn.CrossEntropyLoss()\n",
    "    time_str = str(datetime.now())\n",
    "    time_str = time_str.replace(\" \", \"_\")\n",
    "    time_str = time_str.replace(\":\", \"0\")\n",
    "\n",
    "    # Use os.environ.get() to avoid KeyError\n",
    "    job_id = os.environ.get(\"SLURM_JOB_ID\", \"NA\")\n",
    "\n",
    "    logpath = os.path.join(\n",
    "        logdir,\n",
    "        \"training.log.{}.{}\".format(\n",
    "            job_id, time_str)\n",
    "    )\n",
    "    accpath = logpath + \"_accuracy.txt\"\n",
    "\n",
    "    # Initialize log files\n",
    "    with open(logpath, \"w\") as loglink:\n",
    "        header = [\n",
    "            \"Epoch\",\n",
    "            \"Reconstruction.Loss\",\n",
    "            \"KLD\",\n",
    "            \"CE.Loss\",\n",
    "            \"Accuracy\",\n",
    "            \"MiniBatch.ID\",\n",
    "            \"Time.Stamp\"\n",
    "        ]\n",
    "        loglink.write(\"\\t\".join(header) + \"\\n\")\n",
    "\n",
    "    if predict_celltypes:\n",
    "        with open(accpath, \"a\") as acclink:\n",
    "            header_acc = [\"Epoch\"] + [state + \".acc\" for state in states]\n",
    "            acclink.write(\"\\t\".join(header_acc) + \"\\n\")\n",
    "\n",
    "    TOTBATCHIDX = int(np.ceil(expar.shape[0] / MINIBATCH))\n",
    "\n",
    "    for epoch in range(MAXEPOCH):\n",
    "        # Shuffle indices for each epoch\n",
    "        sampled_idxs = np.random.choice(\n",
    "            np.arange(expar.shape[0]), expar.shape[0], replace=False\n",
    "        )\n",
    "\n",
    "        running_loss_reconst = 0\n",
    "        running_kld = 0\n",
    "        running_ce = 0\n",
    "        running_loss = 0\n",
    "        accval = 0\n",
    "        state_resps = np.zeros(expar.shape[0])\n",
    "        state_preds = np.zeros(expar.shape[0])\n",
    "\n",
    "        for idxbatch in range(TOTBATCHIDX):\n",
    "            idxbatch_st = idxbatch * MINIBATCH\n",
    "            idxbatch_end = min((idxbatch + 1) * MINIBATCH, expar.shape[0])\n",
    "            cur_sidxs = sampled_idxs[idxbatch_st:idxbatch_end]\n",
    "            train1 = expar[cur_sidxs, :]\n",
    "\n",
    "            if batch_idxs is not None:\n",
    "                batch_idxs_tensor = torch.from_numpy(batch_idxs[cur_sidxs]).long().to(device).reshape(-1, 1)\n",
    "            else:\n",
    "                batch_idxs_tensor = None\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outdict = vae(train1)\n",
    "            recon_x = outdict['recon_x']\n",
    "            mu = outdict['mu']\n",
    "            logvar = outdict['logvar']\n",
    "\n",
    "            if predict_celltypes:\n",
    "                state_pred = outdict['state_pred']\n",
    "                state_true = state_labels[cur_sidxs]\n",
    "            else:\n",
    "                state_pred = None\n",
    "                state_true = None\n",
    "\n",
    "            loss, loss_1, loss_2, loss_3 = loss_function(\n",
    "                recon_x, train1, mu, logvar,\n",
    "                state_pred=state_pred, state_true=state_true,\n",
    "                loss_scalers=loss_scalers\n",
    "            )\n",
    "\n",
    "            # Check for NaNs\n",
    "            if torch.isnan(loss):\n",
    "                print(f\"Losses: Reconstruction Loss = {loss_1.item()}, KLD = {loss_2.item()}, CE Loss = {loss_3.item()}\")\n",
    "                print(\"Checking for NaNs in recon_x, x, mu, logvar, state_pred, state_true\")\n",
    "                print(f\"NaNs in recon_x: {torch.isnan(recon_x).any()}\")\n",
    "                print(f\"NaNs in x: {torch.isnan(train1).any()}\")\n",
    "                print(f\"NaNs in mu: {torch.isnan(mu).any()}\")\n",
    "                print(f\"NaNs in logvar: {torch.isnan(logvar).any()}\")\n",
    "                if predict_celltypes:\n",
    "                    print(f\"NaNs in state_pred: {torch.isnan(state_pred).any()}\")\n",
    "                    print(f\"NaNs in state_true: {torch.isnan(state_true).any()}\")\n",
    "                raise ValueError(\"NaN occurred in loss computation.\")\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(vae.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss_reconst += loss_1.item() / loss_scalers[0]\n",
    "            running_kld += loss_2.item() / loss_scalers[1]\n",
    "            running_ce += loss_3.item() / loss_scalers[2]\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if predict_celltypes:\n",
    "                _, predicted = torch.max(state_pred.data, 1)\n",
    "                total = state_true.size(0)\n",
    "                correct = (predicted == state_true).sum().item()\n",
    "                accval += correct / total\n",
    "                state_resps[cur_sidxs] = state_true.cpu().numpy()\n",
    "                state_preds[cur_sidxs] = predicted.cpu().numpy()\n",
    "\n",
    "            del train1, outdict\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        cur_loss = running_loss / TOTBATCHIDX\n",
    "        cur_loss_reconst = running_loss_reconst / TOTBATCHIDX\n",
    "        cur_kld = running_kld / TOTBATCHIDX\n",
    "        cur_ce = running_ce / TOTBATCHIDX\n",
    "        accval = accval / TOTBATCHIDX\n",
    "\n",
    "        adlist_cts = [str(epoch)]\n",
    "        for k in range(len(states)):\n",
    "            pred_state = state_preds == k\n",
    "            resp_state = state_resps == k\n",
    "            cur_acc = accuracy_score(resp_state, pred_state)\n",
    "            adlist_cts.append(str(round(cur_acc, 3)))\n",
    "\n",
    "        if predict_celltypes:\n",
    "            with open(accpath, \"a\") as acclink:\n",
    "                acclink.write(\"\\t\".join(adlist_cts) + \"\\n\")\n",
    "\n",
    "        print(f\"Epoch {epoch}, Loss {cur_loss} at {datetime.now()}\")\n",
    "\n",
    "        with open(logpath, \"a\") as loglink:\n",
    "            adlist = [\n",
    "                str(epoch),\n",
    "                str(cur_loss_reconst),\n",
    "                str(cur_kld),\n",
    "                str(cur_ce),\n",
    "                str(round(accval, 3)),\n",
    "                str(idxbatch),\n",
    "                str(datetime.now())\n",
    "            ]\n",
    "            loglink.write(\"\\t\".join(adlist) + \"\\n\")\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            checkpoint = {\n",
    "                'model': vae.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }\n",
    "            for eachpath in [modelpath, chkpath]:\n",
    "                torch.save(checkpoint, eachpath)\n",
    "\n",
    "    return vae\n",
    "\n",
    "def load_inputs(nparpaths, gmtmat, outdir, genes, metapaths):\n",
    "    \"\"\"\n",
    "    Loads the input data from AnnData HDF5 files and prepares it for training.\n",
    "    \"\"\"\n",
    "    expar_list = []\n",
    "    barcodes_list = []\n",
    "    metadf_list = []\n",
    "\n",
    "    for i in range(len(nparpaths)):\n",
    "        print(f\"Loading {nparpaths[i]}\")\n",
    "        # Load the AnnData object\n",
    "        adata = sc.read_h5ad(nparpaths[i])\n",
    "\n",
    "        # Extract expression matrix\n",
    "        expar = adata.X\n",
    "        if sp_sparse.issparse(expar):\n",
    "            expar = expar.toarray()  # Convert to dense if necessary\n",
    "\n",
    "        # Extract gene names\n",
    "        if 'Gene' in adata.var.columns:\n",
    "            genes = adata.var['Gene'].values\n",
    "        else:\n",
    "            genes = adata.var.index.values\n",
    "\n",
    "        # Extract cell barcodes\n",
    "        if 'CellID' in adata.obs.columns:\n",
    "            barcodes = adata.obs['CellID'].values\n",
    "        else:\n",
    "            barcodes = adata.obs.index.values\n",
    "\n",
    "        # Extract cell metadata\n",
    "        metadf = adata.obs.copy()\n",
    "\n",
    "        # Append to lists\n",
    "        expar_list.append(expar)\n",
    "        barcodes_list.append(barcodes)\n",
    "        metadf_list.append(metadf)\n",
    "\n",
    "    # Concatenate data from all datasets\n",
    "    expar = np.concatenate(expar_list, axis=0)\n",
    "    barcodes = np.concatenate(barcodes_list, axis=0)\n",
    "    metadf = pd.concat(metadf_list, axis=0, ignore_index=True)\n",
    "\n",
    "    # Return the data in a dictionary\n",
    "    return {\n",
    "        'expar': expar,\n",
    "        'metadf': metadf,\n",
    "        'barcodes': barcodes,\n",
    "        'genes': genes,\n",
    "        'gmtmat': gmtmat\n",
    "    }\n",
    "\n",
    "def load_existing_model(modelpath, chkpath, vae, optimizer):\n",
    "    \"\"\"\n",
    "    Loads an existing model and optimizer state.\n",
    "    \"\"\"\n",
    "    for eachpath in [modelpath, chkpath]:\n",
    "        if os.path.exists(eachpath):\n",
    "            try:\n",
    "                checkpoint = torch.load(eachpath)\n",
    "                state_dict = checkpoint['model']\n",
    "                new_state_dict = OrderedDict()\n",
    "                for k, v in state_dict.items():\n",
    "                    k = k.replace('module.', '')\n",
    "                    new_state_dict[k] = v\n",
    "                vae.load_state_dict(new_state_dict)\n",
    "                optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "                print(f\"Loaded from {eachpath}\")\n",
    "                return vae, optimizer\n",
    "            except Exception:\n",
    "                pass\n",
    "    print(\"Didn't load from any\")\n",
    "    return vae, optimizer\n",
    "\n",
    "def make_gmtmat(gmtpath, outdir, genepath):\n",
    "    \"\"\"\n",
    "    Loads the GMT matrix and gene list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gmtpath : str\n",
    "        Path to the GMT file containing gene set definitions.\n",
    "    outdir : str\n",
    "        Output directory to save intermediate files.\n",
    "    genepath : str\n",
    "        Path to the file containing gene names.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    gmtmat : np.ndarray\n",
    "        Gene-TF connection matrix (genes x TFs).\n",
    "    tfs : list\n",
    "        List of transcription factor names.\n",
    "    genes_gmtmat : list\n",
    "        List of gene names used in gmtmat.\n",
    "    \"\"\"\n",
    "    # Load the gene list from 'genepath'\n",
    "    with open(genepath, 'r') as f:\n",
    "        genes_gmtmat = [line.strip() for line in f]\n",
    "\n",
    "    # Load the GMT file and parse TFs\n",
    "    tfs = []\n",
    "    tf_gene_dict = {}\n",
    "    with open(gmtpath, 'r') as gmt_file:\n",
    "        for line in gmt_file:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            tf_name = tokens[0]\n",
    "            target_genes = tokens[2:]  # Skip description in tokens[1]\n",
    "            tfs.append(tf_name)\n",
    "            tf_gene_dict[tf_name] = set(target_genes)\n",
    "\n",
    "    n_tfs = len(tfs)\n",
    "    n_genes = len(genes_gmtmat)\n",
    "\n",
    "    # Initialize gmtmat with zeros\n",
    "    gmtmat = np.zeros((n_genes, n_tfs), dtype=np.float32)\n",
    "\n",
    "    gene_to_index = {gene: idx for idx, gene in enumerate(genes_gmtmat)}\n",
    "\n",
    "    for tf_idx, tf_name in enumerate(tfs):\n",
    "        target_genes = tf_gene_dict[tf_name]\n",
    "        for gene in target_genes:\n",
    "            if gene in gene_to_index:\n",
    "                gene_idx = gene_to_index[gene]\n",
    "                gmtmat[gene_idx, tf_idx] = 1  # Binary connections\n",
    "\n",
    "    return gmtmat, tfs, genes_gmtmat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a530821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "#from model import VAE, loss_function  # Make sure model.py is in the same directory\n",
    "#from train import main  # Make sure train.py is in the same directory\n",
    "import h5py\n",
    "import scvelo as scv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "262d1be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/Users/brendamelano/opt/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/warnings.py:53: LightningDeprecationWarning: pytorch_lightning.utilities.warnings.rank_zero_deprecation has been deprecated in v1.6 and will be removed in v1.8. Use the equivalent function from the pytorch_lightning.utilities.rank_zero module instead.\n",
      "  new_rank_zero_deprecation(\n",
      "/Users/brendamelano/opt/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/warnings.py:58: LightningDeprecationWarning: The `pytorch_lightning.loggers.base.rank_zero_experiment` is deprecated in v1.7 and will be removed in v1.9. Please use `pytorch_lightning.loggers.logger.rank_zero_experiment` instead.\n",
      "  return new_rank_zero_deprecation(*args, **kwargs)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21bf44d3",
   "metadata": {},
   "source": [
    "# Running MAVE on OS152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee9a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: OS152\n",
      "Bootstrap iteration: 1/5\n",
      "Loading /Users/brendamelano/Desktop/Reprogramming_Osteosarcoma/plain_scRNAseq_analysis/OS152/OS152_adata_subtype_PCA.h5ad\n",
      "Epoch 0, Loss 209.85550914091223 at 2024-10-08 11:31:38.107602\n",
      "Epoch 1, Loss 197.8373923956179 at 2024-10-08 11:31:43.131246\n",
      "Epoch 2, Loss 195.7526246613147 at 2024-10-08 11:31:47.899802\n",
      "Epoch 3, Loss 194.6672815061083 at 2024-10-08 11:31:53.338234\n",
      "Epoch 4, Loss 193.8763753853592 at 2024-10-08 11:31:59.912385\n",
      "Epoch 5, Loss 193.14154411764707 at 2024-10-08 11:32:05.053773\n",
      "Epoch 6, Loss 192.5203742980957 at 2024-10-08 11:32:10.371475\n",
      "Epoch 7, Loss 192.0291124231675 at 2024-10-08 11:32:15.223442\n",
      "Epoch 8, Loss 191.5847600301107 at 2024-10-08 11:32:20.055241\n",
      "Epoch 9, Loss 191.17907318414427 at 2024-10-08 11:32:25.071320\n",
      "Epoch 10, Loss 190.8751890145096 at 2024-10-08 11:32:30.010517\n",
      "Epoch 11, Loss 190.64690159816368 at 2024-10-08 11:32:35.401424\n",
      "Epoch 12, Loss 190.43299237419578 at 2024-10-08 11:32:40.381388\n",
      "Epoch 13, Loss 190.25794160132315 at 2024-10-08 11:32:45.378329\n",
      "Epoch 14, Loss 190.09609820795995 at 2024-10-08 11:32:50.249358\n",
      "Epoch 15, Loss 189.9978977278167 at 2024-10-08 11:32:55.181976\n",
      "Epoch 16, Loss 189.89521183687097 at 2024-10-08 11:33:00.032460\n",
      "Epoch 17, Loss 189.80652281817268 at 2024-10-08 11:33:04.880239\n",
      "Epoch 18, Loss 189.7267489713781 at 2024-10-08 11:33:09.753216\n",
      "Epoch 19, Loss 189.65683536903532 at 2024-10-08 11:33:14.636847\n",
      "Epoch 20, Loss 189.61101913452148 at 2024-10-08 11:33:19.599138\n",
      "Epoch 21, Loss 189.5567642660702 at 2024-10-08 11:33:24.693978\n",
      "Epoch 22, Loss 189.49205435958564 at 2024-10-08 11:33:29.746333\n",
      "Epoch 23, Loss 189.46191159416648 at 2024-10-08 11:33:37.062810\n",
      "Epoch 24, Loss 189.4312836890127 at 2024-10-08 11:33:42.325928\n",
      "Epoch 25, Loss 189.39239240160177 at 2024-10-08 11:33:47.699993\n",
      "Epoch 26, Loss 189.34815245983648 at 2024-10-08 11:33:53.245634\n",
      "Epoch 27, Loss 189.320193122415 at 2024-10-08 11:33:58.487120\n",
      "Epoch 28, Loss 189.29929501402611 at 2024-10-08 11:34:03.754666\n",
      "Epoch 29, Loss 189.2740385018143 at 2024-10-08 11:34:09.935647\n",
      "Epoch 30, Loss 189.2455182542988 at 2024-10-08 11:34:15.350657\n",
      "Epoch 31, Loss 189.21735030529547 at 2024-10-08 11:34:21.005520\n",
      "Epoch 32, Loss 189.1926975624234 at 2024-10-08 11:34:26.323039\n",
      "Epoch 33, Loss 189.17145224178537 at 2024-10-08 11:34:31.665140\n",
      "Epoch 34, Loss 189.1625316096287 at 2024-10-08 11:34:37.168159\n",
      "Epoch 35, Loss 189.13919097302008 at 2024-10-08 11:34:42.720372\n",
      "Epoch 36, Loss 189.11844896802714 at 2024-10-08 11:34:48.011361\n",
      "Epoch 37, Loss 189.09229570276597 at 2024-10-08 11:34:53.411893\n",
      "Epoch 38, Loss 189.0792823492312 at 2024-10-08 11:34:58.825104\n",
      "Epoch 39, Loss 189.06430786731195 at 2024-10-08 11:35:04.333258\n",
      "Epoch 40, Loss 189.04856715482825 at 2024-10-08 11:35:09.729934\n",
      "Epoch 41, Loss 189.0277478835162 at 2024-10-08 11:35:15.354167\n",
      "Epoch 42, Loss 189.00945065068262 at 2024-10-08 11:35:20.496328\n",
      "Epoch 43, Loss 188.99315994861078 at 2024-10-08 11:35:25.638815\n",
      "Epoch 44, Loss 188.99263404397402 at 2024-10-08 11:35:31.337111\n",
      "Epoch 45, Loss 188.96974197088502 at 2024-10-08 11:35:37.864073\n",
      "Epoch 46, Loss 188.96321457507563 at 2024-10-08 11:35:43.894339\n",
      "Epoch 47, Loss 188.94076141656615 at 2024-10-08 11:35:49.155505\n",
      "Epoch 48, Loss 188.93757674273323 at 2024-10-08 11:35:54.430184\n",
      "Epoch 49, Loss 188.9177842981675 at 2024-10-08 11:35:59.443184\n",
      "Bootstrap iteration: 2/5\n",
      "Loading /Users/brendamelano/Desktop/Reprogramming_Osteosarcoma/plain_scRNAseq_analysis/OS152/OS152_adata_subtype_PCA.h5ad\n",
      "Epoch 0, Loss 210.7883519191368 at 2024-10-08 11:36:06.245995\n",
      "Epoch 1, Loss 198.1025385389141 at 2024-10-08 11:36:12.441860\n",
      "Epoch 2, Loss 196.04919620588714 at 2024-10-08 11:36:17.419007\n",
      "Epoch 3, Loss 194.89873482199278 at 2024-10-08 11:36:22.194308\n",
      "Epoch 4, Loss 194.1049789727903 at 2024-10-08 11:36:27.132055\n",
      "Epoch 5, Loss 193.35957231708602 at 2024-10-08 11:36:31.732039\n",
      "Epoch 6, Loss 192.75709735645967 at 2024-10-08 11:36:36.669740\n",
      "Epoch 7, Loss 192.21756340475645 at 2024-10-08 11:36:41.518868\n",
      "Epoch 8, Loss 191.77885870840035 at 2024-10-08 11:36:46.581448\n",
      "Epoch 9, Loss 191.39453723383886 at 2024-10-08 11:36:51.305678\n",
      "Epoch 10, Loss 191.07069815841376 at 2024-10-08 11:36:56.242917\n",
      "Epoch 11, Loss 190.82526113472733 at 2024-10-08 11:37:01.788674\n",
      "Epoch 12, Loss 190.5985487395642 at 2024-10-08 11:37:07.439235\n",
      "Epoch 13, Loss 190.43157839307597 at 2024-10-08 11:37:12.768000\n",
      "Epoch 14, Loss 190.27778640447877 at 2024-10-08 11:37:17.403098\n",
      "Epoch 15, Loss 190.14514945535097 at 2024-10-08 11:37:22.190746\n",
      "Epoch 16, Loss 190.04346832574583 at 2024-10-08 11:37:26.771013\n",
      "Epoch 17, Loss 189.95984993728936 at 2024-10-08 11:37:31.354837\n",
      "Epoch 18, Loss 189.8705918555166 at 2024-10-08 11:37:36.437429\n",
      "Epoch 19, Loss 189.79413208307005 at 2024-10-08 11:37:41.218730\n",
      "Epoch 20, Loss 189.72972652958887 at 2024-10-08 11:37:45.830053\n",
      "Epoch 21, Loss 189.6931889851888 at 2024-10-08 11:37:50.873718\n",
      "Epoch 22, Loss 189.62814390893075 at 2024-10-08 11:37:55.499946\n",
      "Epoch 23, Loss 189.59208477244658 at 2024-10-08 11:38:00.110815\n",
      "Epoch 24, Loss 189.5568717507755 at 2024-10-08 11:38:04.969562\n",
      "Epoch 25, Loss 189.50310187246285 at 2024-10-08 11:38:10.064215\n",
      "Epoch 26, Loss 189.47973812327666 at 2024-10-08 11:38:15.421168\n",
      "Epoch 27, Loss 189.4444610745299 at 2024-10-08 11:38:20.690034\n",
      "Epoch 28, Loss 189.40311678718118 at 2024-10-08 11:38:25.916377\n",
      "Epoch 29, Loss 189.38940317490523 at 2024-10-08 11:38:30.863461\n",
      "Epoch 30, Loss 189.34964123894187 at 2024-10-08 11:38:36.520655\n",
      "Epoch 31, Loss 189.32770538330078 at 2024-10-08 11:38:41.682908\n",
      "Epoch 32, Loss 189.30122816796396 at 2024-10-08 11:38:46.494475\n",
      "Epoch 33, Loss 189.27008378271964 at 2024-10-08 11:38:51.306619\n",
      "Epoch 34, Loss 189.2521053389007 at 2024-10-08 11:38:56.119028\n",
      "Epoch 35, Loss 189.2318395726821 at 2024-10-08 11:39:00.815451\n",
      "Epoch 36, Loss 189.21123295204313 at 2024-10-08 11:39:05.451878\n",
      "Epoch 37, Loss 189.19749704996744 at 2024-10-08 11:39:10.123714\n",
      "Epoch 38, Loss 189.1714453603707 at 2024-10-08 11:39:14.978491\n",
      "Epoch 39, Loss 189.1383067112343 at 2024-10-08 11:39:19.739976\n",
      "Epoch 40, Loss 189.12287005256204 at 2024-10-08 11:39:24.551533\n",
      "Epoch 41, Loss 189.10906002568262 at 2024-10-08 11:39:29.653488\n",
      "Epoch 42, Loss 189.09935132194968 at 2024-10-08 11:39:35.157809\n",
      "Epoch 43, Loss 189.0821771808699 at 2024-10-08 11:39:40.845223\n",
      "Epoch 44, Loss 189.06019038780062 at 2024-10-08 11:39:45.931940\n",
      "Epoch 45, Loss 189.04921901927275 at 2024-10-08 11:39:51.000059\n",
      "Epoch 46, Loss 189.02783704271505 at 2024-10-08 11:39:56.206687\n",
      "Epoch 47, Loss 189.01719605688956 at 2024-10-08 11:40:01.277552\n",
      "Epoch 48, Loss 189.0059492073807 at 2024-10-08 11:40:06.267684\n",
      "Epoch 49, Loss 188.98760275747262 at 2024-10-08 11:40:11.172617\n",
      "Bootstrap iteration: 3/5\n",
      "Loading /Users/brendamelano/Desktop/Reprogramming_Osteosarcoma/plain_scRNAseq_analysis/OS152/OS152_adata_subtype_PCA.h5ad\n",
      "Epoch 0, Loss 210.1844731499167 at 2024-10-08 11:40:17.423427\n",
      "Epoch 1, Loss 197.9114647659601 at 2024-10-08 11:40:22.926801\n",
      "Epoch 2, Loss 195.8733861586627 at 2024-10-08 11:40:28.116892\n",
      "Epoch 3, Loss 194.78826455508963 at 2024-10-08 11:40:33.711421\n",
      "Epoch 4, Loss 193.90448289759019 at 2024-10-08 11:40:39.462379\n",
      "Epoch 5, Loss 193.19145240035712 at 2024-10-08 11:40:44.699564\n",
      "Epoch 6, Loss 192.6062504637475 at 2024-10-08 11:40:50.108571\n",
      "Epoch 7, Loss 192.07258882709579 at 2024-10-08 11:40:56.358385\n",
      "Epoch 8, Loss 191.62696075439453 at 2024-10-08 11:41:01.760052\n",
      "Epoch 9, Loss 191.25108576755898 at 2024-10-08 11:41:06.790513\n",
      "Epoch 10, Loss 190.9564229179831 at 2024-10-08 11:41:12.314747\n",
      "Epoch 11, Loss 190.6893113828173 at 2024-10-08 11:41:17.329343\n",
      "Epoch 12, Loss 190.47774109185912 at 2024-10-08 11:41:22.118710\n",
      "Epoch 13, Loss 190.28449974808038 at 2024-10-08 11:41:26.855425\n",
      "Epoch 14, Loss 190.14821953866996 at 2024-10-08 11:41:31.579615\n",
      "Epoch 15, Loss 190.0306593951057 at 2024-10-08 11:41:37.437483\n",
      "Epoch 16, Loss 189.93254126754462 at 2024-10-08 11:41:42.510038\n",
      "Epoch 17, Loss 189.83465897803214 at 2024-10-08 11:41:47.507224\n",
      "Epoch 18, Loss 189.76652025708964 at 2024-10-08 11:41:52.522043\n",
      "Epoch 19, Loss 189.7028985116996 at 2024-10-08 11:41:57.437403\n",
      "Epoch 20, Loss 189.63623944450828 at 2024-10-08 11:42:02.339487\n",
      "Epoch 21, Loss 189.58629107007792 at 2024-10-08 11:42:07.702703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Loss 189.53717549641928 at 2024-10-08 11:42:12.594240\n",
      "Epoch 23, Loss 189.50076383702896 at 2024-10-08 11:42:17.325624\n",
      "Epoch 24, Loss 189.4604226654651 at 2024-10-08 11:42:22.082865\n",
      "Epoch 25, Loss 189.42577175065583 at 2024-10-08 11:42:27.069373\n",
      "Epoch 26, Loss 189.39593333824007 at 2024-10-08 11:42:32.312110\n",
      "Epoch 27, Loss 189.35771785062903 at 2024-10-08 11:42:38.187603\n",
      "Epoch 28, Loss 189.3230701521331 at 2024-10-08 11:42:43.398162\n",
      "Epoch 29, Loss 189.30475130268172 at 2024-10-08 11:42:48.546531\n",
      "Epoch 30, Loss 189.27764324113434 at 2024-10-08 11:42:54.270519\n",
      "Epoch 31, Loss 189.25508716059667 at 2024-10-08 11:42:59.593366\n",
      "Epoch 32, Loss 189.21683988384171 at 2024-10-08 11:43:04.931194\n",
      "Epoch 33, Loss 189.2094035429113 at 2024-10-08 11:43:10.243494\n",
      "Epoch 34, Loss 189.1825013254203 at 2024-10-08 11:43:15.385942\n",
      "Epoch 35, Loss 189.16685373642866 at 2024-10-08 11:43:20.551246\n",
      "Epoch 36, Loss 189.1439037696988 at 2024-10-08 11:43:25.581154\n",
      "Epoch 37, Loss 189.1276984869265 at 2024-10-08 11:43:31.157167\n",
      "Epoch 38, Loss 189.10176729688456 at 2024-10-08 11:43:36.905096\n",
      "Epoch 39, Loss 189.08652638454063 at 2024-10-08 11:43:42.189682\n",
      "Epoch 40, Loss 189.0720453449324 at 2024-10-08 11:43:47.232300\n",
      "Epoch 41, Loss 189.05249554503197 at 2024-10-08 11:43:52.884375\n",
      "Epoch 42, Loss 189.03693891039083 at 2024-10-08 11:43:58.153833\n",
      "Epoch 43, Loss 189.02228209551643 at 2024-10-08 11:44:03.397474\n",
      "Epoch 44, Loss 189.00689240997912 at 2024-10-08 11:44:08.941657\n",
      "Epoch 45, Loss 188.9996972177543 at 2024-10-08 11:44:14.119491\n",
      "Epoch 46, Loss 188.9822856678682 at 2024-10-08 11:44:19.167052\n",
      "Epoch 47, Loss 188.96377922506895 at 2024-10-08 11:44:24.346459\n",
      "Epoch 48, Loss 188.956049526439 at 2024-10-08 11:44:29.906247\n",
      "Epoch 49, Loss 188.9409735436533 at 2024-10-08 11:44:35.862825\n",
      "Bootstrap iteration: 4/5\n",
      "Loading /Users/brendamelano/Desktop/Reprogramming_Osteosarcoma/plain_scRNAseq_analysis/OS152/OS152_adata_subtype_PCA.h5ad\n",
      "Epoch 0, Loss 210.32935183655982 at 2024-10-08 11:44:42.372814\n",
      "Epoch 1, Loss 197.96568649890375 at 2024-10-08 11:44:48.084167\n",
      "Epoch 2, Loss 195.86606777415557 at 2024-10-08 11:44:53.281773\n",
      "Epoch 3, Loss 194.75319192923752 at 2024-10-08 11:44:58.513137\n",
      "Epoch 4, Loss 193.89635325413124 at 2024-10-08 11:45:03.504213\n",
      "Epoch 5, Loss 193.16318549361884 at 2024-10-08 11:45:08.710258\n",
      "Epoch 6, Loss 192.57770186779547 at 2024-10-08 11:45:14.771238\n",
      "Epoch 7, Loss 192.03052034565047 at 2024-10-08 11:45:20.007577\n",
      "Epoch 8, Loss 191.59113438924155 at 2024-10-08 11:45:24.906878\n",
      "Epoch 9, Loss 191.2268207774443 at 2024-10-08 11:45:29.799596\n",
      "Epoch 10, Loss 190.90114189596738 at 2024-10-08 11:45:35.266450\n",
      "Epoch 11, Loss 190.64673449946386 at 2024-10-08 11:45:41.321231\n",
      "Epoch 12, Loss 190.42271894567153 at 2024-10-08 11:45:46.543710\n",
      "Epoch 13, Loss 190.2404270546109 at 2024-10-08 11:45:51.522231\n",
      "Epoch 14, Loss 190.11498986038507 at 2024-10-08 11:45:57.483635\n",
      "Epoch 15, Loss 189.99441528320312 at 2024-10-08 11:46:03.557219\n",
      "Epoch 16, Loss 189.89006498748182 at 2024-10-08 11:46:08.740537\n",
      "Epoch 17, Loss 189.8141086054783 at 2024-10-08 11:46:13.420324\n",
      "Epoch 18, Loss 189.7308422163421 at 2024-10-08 11:46:18.097065\n",
      "Epoch 19, Loss 189.68342926922966 at 2024-10-08 11:46:22.773893\n",
      "Epoch 20, Loss 189.60300587672813 at 2024-10-08 11:46:27.655830\n",
      "Epoch 21, Loss 189.55554902319815 at 2024-10-08 11:46:32.584764\n",
      "Epoch 22, Loss 189.50920636046166 at 2024-10-08 11:46:38.685628\n",
      "Epoch 23, Loss 189.45000270768708 at 2024-10-08 11:46:43.602123\n",
      "Epoch 24, Loss 189.42305381625306 at 2024-10-08 11:46:48.451700\n",
      "Epoch 25, Loss 189.38846887326707 at 2024-10-08 11:46:53.344745\n",
      "Epoch 26, Loss 189.36385225782206 at 2024-10-08 11:46:58.323305\n",
      "Epoch 27, Loss 189.3194549410951 at 2024-10-08 11:47:03.451578\n",
      "Epoch 28, Loss 189.30304014916513 at 2024-10-08 11:47:08.759022\n",
      "Epoch 29, Loss 189.27929074156518 at 2024-10-08 11:47:13.712277\n",
      "Epoch 30, Loss 189.23838222728057 at 2024-10-08 11:47:18.725567\n",
      "Epoch 31, Loss 189.21689673031076 at 2024-10-08 11:47:24.167914\n",
      "Epoch 32, Loss 189.2081977993834 at 2024-10-08 11:47:29.091973\n",
      "Epoch 33, Loss 189.17862544340247 at 2024-10-08 11:47:34.917484\n",
      "Epoch 34, Loss 189.15758484485102 at 2024-10-08 11:47:41.026639\n",
      "Epoch 35, Loss 189.136516197055 at 2024-10-08 11:47:46.263259\n",
      "Epoch 36, Loss 189.117540770886 at 2024-10-08 11:47:51.611205\n",
      "Epoch 37, Loss 189.0943334242877 at 2024-10-08 11:47:57.006249\n",
      "Epoch 38, Loss 189.08414302152747 at 2024-10-08 11:48:02.989790\n",
      "Epoch 39, Loss 189.06988271077475 at 2024-10-08 11:48:09.061202\n",
      "Epoch 40, Loss 189.04631543626974 at 2024-10-08 11:48:14.854142\n",
      "Epoch 41, Loss 189.03486162073472 at 2024-10-08 11:48:21.103246\n",
      "Epoch 42, Loss 189.0189468533385 at 2024-10-08 11:48:25.985739\n",
      "Epoch 43, Loss 188.99552064783433 at 2024-10-08 11:48:30.745709\n",
      "Epoch 44, Loss 188.98883834539674 at 2024-10-08 11:48:36.497609\n",
      "Epoch 45, Loss 188.96967861699122 at 2024-10-08 11:48:41.743383\n",
      "Epoch 46, Loss 188.95681859932694 at 2024-10-08 11:48:47.049279\n",
      "Epoch 47, Loss 188.9499703949573 at 2024-10-08 11:48:52.303770\n",
      "Epoch 48, Loss 188.93057408052331 at 2024-10-08 11:48:57.551576\n",
      "Epoch 49, Loss 188.91826008815391 at 2024-10-08 11:49:02.917868\n",
      "Bootstrap iteration: 5/5\n",
      "Loading /Users/brendamelano/Desktop/Reprogramming_Osteosarcoma/plain_scRNAseq_analysis/OS152/OS152_adata_subtype_PCA.h5ad\n",
      "Epoch 0, Loss 210.2798844131769 at 2024-10-08 11:49:09.893798\n",
      "Epoch 1, Loss 198.01222012089747 at 2024-10-08 11:49:15.178377\n",
      "Epoch 2, Loss 195.88703222835764 at 2024-10-08 11:49:20.032522\n",
      "Epoch 3, Loss 194.83115095250747 at 2024-10-08 11:49:24.880543\n",
      "Epoch 4, Loss 193.98460537779565 at 2024-10-08 11:49:29.883927\n",
      "Epoch 5, Loss 193.2575471915451 at 2024-10-08 11:49:35.801422\n",
      "Epoch 6, Loss 192.64830009610046 at 2024-10-08 11:49:41.488987\n",
      "Epoch 7, Loss 192.08754999497359 at 2024-10-08 11:49:46.548380\n",
      "Epoch 8, Loss 191.59688239004097 at 2024-10-08 11:49:51.563087\n",
      "Epoch 9, Loss 191.26286659988702 at 2024-10-08 11:49:56.653889\n",
      "Epoch 10, Loss 190.9291810428395 at 2024-10-08 11:50:01.635197\n",
      "Epoch 11, Loss 190.67995273365693 at 2024-10-08 11:50:06.918562\n",
      "Epoch 12, Loss 190.47364126467238 at 2024-10-08 11:50:12.111858\n",
      "Epoch 13, Loss 190.2832753798541 at 2024-10-08 11:50:17.284126\n",
      "Epoch 14, Loss 190.12598090078316 at 2024-10-08 11:50:22.316149\n",
      "Epoch 15, Loss 190.01590908274932 at 2024-10-08 11:50:27.441697\n",
      "Epoch 16, Loss 189.9217767902449 at 2024-10-08 11:50:33.793507\n",
      "Epoch 17, Loss 189.80921547085632 at 2024-10-08 11:50:40.010675\n",
      "Epoch 18, Loss 189.7472360648361 at 2024-10-08 11:50:45.707116\n",
      "Epoch 19, Loss 189.67674188052908 at 2024-10-08 11:50:51.147742\n",
      "Epoch 20, Loss 189.61925573909983 at 2024-10-08 11:50:56.852361\n",
      "Epoch 21, Loss 189.5799654792337 at 2024-10-08 11:51:02.802575\n",
      "Epoch 22, Loss 189.52041678335152 at 2024-10-08 11:51:08.709601\n",
      "Epoch 23, Loss 189.4914160335765 at 2024-10-08 11:51:14.698871\n",
      "Epoch 24, Loss 189.43532958685182 at 2024-10-08 11:51:20.278038\n",
      "Epoch 25, Loss 189.40911169613108 at 2024-10-08 11:51:25.767002\n",
      "Epoch 26, Loss 189.37445704142252 at 2024-10-08 11:51:31.230012\n",
      "Epoch 27, Loss 189.3423537460028 at 2024-10-08 11:51:37.891654\n",
      "Epoch 28, Loss 189.31085309795304 at 2024-10-08 11:51:43.200170\n",
      "Epoch 29, Loss 189.28835842656153 at 2024-10-08 11:51:48.505964\n",
      "Epoch 30, Loss 189.25674677830116 at 2024-10-08 11:51:54.074562\n",
      "Epoch 31, Loss 189.23592257032206 at 2024-10-08 11:52:01.830820\n",
      "Epoch 32, Loss 189.21097190707337 at 2024-10-08 11:52:07.389283\n",
      "Epoch 33, Loss 189.19400495641372 at 2024-10-08 11:52:13.017560\n",
      "Epoch 34, Loss 189.16763642254998 at 2024-10-08 11:52:18.473592\n",
      "Epoch 35, Loss 189.14992298799402 at 2024-10-08 11:52:24.185147\n",
      "Epoch 36, Loss 189.12753512812597 at 2024-10-08 11:52:29.191981\n",
      "Epoch 37, Loss 189.11201544368967 at 2024-10-08 11:52:34.999896\n",
      "Epoch 38, Loss 189.09626111797257 at 2024-10-08 11:52:40.865076\n",
      "Epoch 39, Loss 189.07445915072572 at 2024-10-08 11:52:46.142656\n",
      "Epoch 40, Loss 189.0562167448156 at 2024-10-08 11:52:51.462052\n",
      "Epoch 41, Loss 189.04089998731425 at 2024-10-08 11:52:56.993243\n",
      "Epoch 42, Loss 189.02297704360063 at 2024-10-08 11:53:02.662782\n",
      "Epoch 43, Loss 189.00031617108513 at 2024-10-08 11:53:08.690483\n",
      "Epoch 44, Loss 188.99989431044634 at 2024-10-08 11:53:14.496781\n",
      "Epoch 45, Loss 188.98116826076134 at 2024-10-08 11:53:20.303592\n",
      "Epoch 46, Loss 188.96791278614717 at 2024-10-08 11:53:26.112790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Loss 188.95772492651847 at 2024-10-08 11:53:31.742216\n",
      "Epoch 48, Loss 188.93910179886163 at 2024-10-08 11:53:39.002504\n",
      "Epoch 49, Loss 188.92387292899338 at 2024-10-08 11:53:45.378344\n",
      "Processing dataset: OS384\n",
      "Bootstrap iteration: 1/5\n",
      "Loading /Users/brendamelano/Desktop/Reprogramming_Osteosarcoma/plain_scRNAseq_analysis/OS384/OS384_adata_subtype_PCA.h5ad\n",
      "Epoch 0, Loss 216.42497343175552 at 2024-10-08 11:53:49.542117\n",
      "Epoch 1, Loss 201.6567720899395 at 2024-10-08 11:53:52.755610\n",
      "Epoch 2, Loss 198.8819663851869 at 2024-10-08 11:53:55.707450\n",
      "Epoch 3, Loss 198.13379594391466 at 2024-10-08 11:53:58.604124\n",
      "Epoch 4, Loss 197.54298131606157 at 2024-10-08 11:54:01.420384\n",
      "Epoch 5, Loss 196.95059174182367 at 2024-10-08 11:54:04.219739\n",
      "Epoch 6, Loss 196.508633183498 at 2024-10-08 11:54:07.186960\n",
      "Epoch 7, Loss 196.0842829685585 at 2024-10-08 11:54:10.214764\n",
      "Epoch 8, Loss 195.67930662865732 at 2024-10-08 11:54:13.157655\n",
      "Epoch 9, Loss 195.36234956629136 at 2024-10-08 11:54:17.287162\n",
      "Epoch 10, Loss 195.01596338608687 at 2024-10-08 11:54:20.349462\n",
      "Epoch 11, Loss 194.6942378025429 at 2024-10-08 11:54:23.447253\n",
      "Epoch 12, Loss 194.39177988089767 at 2024-10-08 11:54:26.399009\n",
      "Epoch 13, Loss 194.13639292997473 at 2024-10-08 11:54:29.524460\n",
      "Epoch 14, Loss 193.86061829211664 at 2024-10-08 11:54:32.407158\n",
      "Epoch 15, Loss 193.59287306841682 at 2024-10-08 11:54:37.800143\n",
      "Epoch 16, Loss 193.43869766534544 at 2024-10-08 11:54:41.064065\n",
      "Epoch 17, Loss 193.21967300714232 at 2024-10-08 11:54:43.982312\n",
      "Epoch 18, Loss 193.00606806137984 at 2024-10-08 11:54:46.800827\n",
      "Epoch 19, Loss 192.89810719209558 at 2024-10-08 11:54:49.382578\n",
      "Epoch 20, Loss 192.71158674651502 at 2024-10-08 11:54:51.849170\n",
      "Epoch 21, Loss 192.53394242828966 at 2024-10-08 11:54:54.531797\n",
      "Epoch 22, Loss 192.47609800450942 at 2024-10-08 11:54:57.002659\n",
      "Epoch 23, Loss 192.34603881835938 at 2024-10-08 11:54:59.468523\n",
      "Epoch 24, Loss 192.2249319039139 at 2024-10-08 11:55:01.935150\n",
      "Epoch 25, Loss 192.12648788152956 at 2024-10-08 11:55:04.391660\n",
      "Epoch 26, Loss 192.04111705106848 at 2024-10-08 11:55:06.895221\n",
      "Epoch 27, Loss 191.95345082002527 at 2024-10-08 11:55:10.053216\n",
      "Epoch 28, Loss 191.87913468304802 at 2024-10-08 11:55:12.852738\n",
      "Epoch 29, Loss 191.8468559115541 at 2024-10-08 11:55:15.331963\n",
      "Epoch 30, Loss 191.7893663294175 at 2024-10-08 11:55:17.773820\n",
      "Epoch 31, Loss 191.72750615138634 at 2024-10-08 11:55:20.407142\n",
      "Epoch 32, Loss 191.67580802767884 at 2024-10-08 11:55:22.816936\n",
      "Epoch 33, Loss 191.64049335554535 at 2024-10-08 11:55:25.195658\n",
      "Epoch 34, Loss 191.59075718300016 at 2024-10-08 11:55:27.600468\n",
      "Epoch 35, Loss 191.5562744140625 at 2024-10-08 11:55:30.002720\n",
      "Epoch 36, Loss 191.48911449955958 at 2024-10-08 11:55:32.435914\n",
      "Epoch 37, Loss 191.47269514495252 at 2024-10-08 11:55:36.270600\n",
      "Epoch 38, Loss 191.46019251206343 at 2024-10-08 11:55:39.519011\n",
      "Epoch 39, Loss 191.42416591270296 at 2024-10-08 11:55:42.267339\n",
      "Epoch 40, Loss 191.39440708534391 at 2024-10-08 11:55:44.736381\n",
      "Epoch 41, Loss 191.39496059043734 at 2024-10-08 11:55:47.584957\n",
      "Epoch 42, Loss 191.33798397288604 at 2024-10-08 11:55:50.071669\n",
      "Epoch 43, Loss 191.3474064247281 at 2024-10-08 11:55:52.540449\n",
      "Epoch 44, Loss 191.28512438605813 at 2024-10-08 11:55:55.008164\n",
      "Epoch 45, Loss 191.2947306913488 at 2024-10-08 11:55:57.470310\n",
      "Epoch 46, Loss 191.27432610006895 at 2024-10-08 11:55:59.961753\n",
      "Epoch 47, Loss 191.25198603611366 at 2024-10-08 11:56:02.436903\n",
      "Epoch 48, Loss 191.26131843118105 at 2024-10-08 11:56:04.908408\n",
      "Epoch 49, Loss 191.23805416331572 at 2024-10-08 11:56:07.373646\n",
      "Bootstrap iteration: 2/5\n",
      "Loading /Users/brendamelano/Desktop/Reprogramming_Osteosarcoma/plain_scRNAseq_analysis/OS384/OS384_adata_subtype_PCA.h5ad\n",
      "Epoch 0, Loss 216.07422682818245 at 2024-10-08 11:56:11.008676\n",
      "Epoch 1, Loss 201.67971323050705 at 2024-10-08 11:56:13.659270\n",
      "Epoch 2, Loss 198.86706872079887 at 2024-10-08 11:56:16.093302\n",
      "Epoch 3, Loss 198.08552521350336 at 2024-10-08 11:56:18.522556\n",
      "Epoch 4, Loss 197.47946316588158 at 2024-10-08 11:56:21.171559\n",
      "Epoch 5, Loss 197.0429926853554 at 2024-10-08 11:56:23.611206\n",
      "Epoch 6, Loss 196.5619312361175 at 2024-10-08 11:56:26.039532\n",
      "Epoch 7, Loss 196.1364258410884 at 2024-10-08 11:56:28.396999\n",
      "Epoch 8, Loss 195.71639176910998 at 2024-10-08 11:56:30.737259\n",
      "Epoch 9, Loss 195.38725909064797 at 2024-10-08 11:56:33.248311\n",
      "Epoch 10, Loss 195.00528732000612 at 2024-10-08 11:56:36.049406\n",
      "Epoch 11, Loss 194.77573170381433 at 2024-10-08 11:56:38.900665\n",
      "Epoch 12, Loss 194.4072660558364 at 2024-10-08 11:56:41.667377\n",
      "Epoch 13, Loss 194.15692856732537 at 2024-10-08 11:56:44.262486\n",
      "Epoch 14, Loss 193.90368951535692 at 2024-10-08 11:56:46.687697\n",
      "Epoch 15, Loss 193.6922149658203 at 2024-10-08 11:56:49.139488\n",
      "Epoch 16, Loss 193.4381540335861 at 2024-10-08 11:56:51.571436\n",
      "Epoch 17, Loss 193.25122369504442 at 2024-10-08 11:56:54.016126\n",
      "Epoch 18, Loss 193.0414659088733 at 2024-10-08 11:56:56.496839\n",
      "Epoch 19, Loss 192.90823214661842 at 2024-10-08 11:56:58.916438\n",
      "Epoch 20, Loss 192.75691163306143 at 2024-10-08 11:57:01.350095\n",
      "Epoch 21, Loss 192.6257189582376 at 2024-10-08 11:57:03.974500\n",
      "Epoch 22, Loss 192.46259292901732 at 2024-10-08 11:57:06.455719\n",
      "Epoch 23, Loss 192.34789410759421 at 2024-10-08 11:57:08.938736\n",
      "Epoch 24, Loss 192.23181361778109 at 2024-10-08 11:57:11.444349\n",
      "Epoch 25, Loss 192.14320223939185 at 2024-10-08 11:57:13.917782\n",
      "Epoch 26, Loss 192.05407655005362 at 2024-10-08 11:57:16.382176\n",
      "Epoch 27, Loss 191.97732065238205 at 2024-10-08 11:57:18.821429\n",
      "Epoch 28, Loss 191.92067733465456 at 2024-10-08 11:57:21.256469\n",
      "Epoch 29, Loss 191.85116996017157 at 2024-10-08 11:57:23.701129\n",
      "Epoch 30, Loss 191.8046605727252 at 2024-10-08 11:57:26.147007\n",
      "Epoch 31, Loss 191.72910862342985 at 2024-10-08 11:57:28.743979\n",
      "Epoch 32, Loss 191.6996878829657 at 2024-10-08 11:57:31.216711\n",
      "Epoch 33, Loss 191.64568194221047 at 2024-10-08 11:57:34.740179\n",
      "Epoch 34, Loss 191.60942287071077 at 2024-10-08 11:57:38.496150\n",
      "Epoch 35, Loss 191.57740125469132 at 2024-10-08 11:57:42.624940\n",
      "Epoch 36, Loss 191.53297633750765 at 2024-10-08 11:57:47.951848\n",
      "Epoch 37, Loss 191.49172539804496 at 2024-10-08 11:57:51.375559\n",
      "Epoch 38, Loss 191.47600749894684 at 2024-10-08 11:57:54.637726\n",
      "Epoch 39, Loss 191.43462207270605 at 2024-10-08 11:57:57.396626\n",
      "Epoch 40, Loss 191.43273686427696 at 2024-10-08 11:58:00.044848\n",
      "Epoch 41, Loss 191.3638646742877 at 2024-10-08 11:58:02.918250\n",
      "Epoch 42, Loss 191.34110574161306 at 2024-10-08 11:58:05.517752\n",
      "Epoch 43, Loss 191.35523418351715 at 2024-10-08 11:58:08.249554\n",
      "Epoch 44, Loss 191.3046210793888 at 2024-10-08 11:58:10.921211\n",
      "Epoch 45, Loss 191.30143229166666 at 2024-10-08 11:58:13.533226\n",
      "Epoch 46, Loss 191.29365689146752 at 2024-10-08 11:58:16.384733\n",
      "Epoch 47, Loss 191.26912315219056 at 2024-10-08 11:58:19.494961\n",
      "Epoch 48, Loss 191.2536812576593 at 2024-10-08 11:58:22.161422\n",
      "Epoch 49, Loss 191.2313429888557 at 2024-10-08 11:58:24.985873\n",
      "Bootstrap iteration: 3/5\n",
      "Loading /Users/brendamelano/Desktop/Reprogramming_Osteosarcoma/plain_scRNAseq_analysis/OS384/OS384_adata_subtype_PCA.h5ad\n",
      "Epoch 0, Loss 216.41540168313418 at 2024-10-08 11:58:28.731816\n",
      "Epoch 1, Loss 201.7884401807598 at 2024-10-08 11:58:31.784891\n",
      "Epoch 2, Loss 198.88935223747703 at 2024-10-08 11:58:35.438731\n",
      "Epoch 3, Loss 198.10427108465456 at 2024-10-08 11:58:38.380390\n",
      "Epoch 4, Loss 197.5837198893229 at 2024-10-08 11:58:41.266506\n",
      "Epoch 5, Loss 196.95179374545228 at 2024-10-08 11:58:44.361941\n",
      "Epoch 6, Loss 196.60389649634268 at 2024-10-08 11:58:47.175403\n",
      "Epoch 7, Loss 196.16087820015701 at 2024-10-08 11:58:49.722749\n",
      "Epoch 8, Loss 195.72657835717294 at 2024-10-08 11:58:52.196138\n",
      "Epoch 9, Loss 195.3650001077091 at 2024-10-08 11:58:54.882715\n",
      "Epoch 10, Loss 195.05190830604704 at 2024-10-08 11:58:57.585917\n",
      "Epoch 11, Loss 194.74693866804535 at 2024-10-08 11:59:00.371724\n",
      "Epoch 12, Loss 194.42814546472886 at 2024-10-08 11:59:02.990466\n",
      "Epoch 13, Loss 194.13972981770834 at 2024-10-08 11:59:05.969737\n",
      "Epoch 14, Loss 193.89629349054076 at 2024-10-08 11:59:08.705534\n",
      "Epoch 15, Loss 193.67054120232078 at 2024-10-08 11:59:11.369442\n",
      "Epoch 16, Loss 193.40535152659697 at 2024-10-08 11:59:15.056866\n",
      "Epoch 17, Loss 193.26357284246706 at 2024-10-08 11:59:17.925425\n",
      "Epoch 18, Loss 193.07401081160003 at 2024-10-08 11:59:20.426824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss 192.88334835276885 at 2024-10-08 11:59:22.890420\n",
      "Epoch 20, Loss 192.7198851342295 at 2024-10-08 11:59:25.358319\n",
      "Epoch 21, Loss 192.5856460870481 at 2024-10-08 11:59:28.016064\n",
      "Epoch 22, Loss 192.45180421717026 at 2024-10-08 11:59:30.513541\n",
      "Epoch 23, Loss 192.31359803442862 at 2024-10-08 11:59:33.268190\n",
      "Epoch 24, Loss 192.21567550359987 at 2024-10-08 11:59:36.404464\n",
      "Epoch 25, Loss 192.13661762312347 at 2024-10-08 11:59:39.255031\n",
      "Epoch 26, Loss 192.0325087005017 at 2024-10-08 11:59:42.316969\n",
      "Epoch 27, Loss 191.96672147863052 at 2024-10-08 11:59:45.042822\n",
      "Epoch 28, Loss 191.8806032666973 at 2024-10-08 11:59:47.565990\n",
      "Epoch 29, Loss 191.83342668121935 at 2024-10-08 11:59:50.194684\n",
      "Epoch 30, Loss 191.76276652018228 at 2024-10-08 11:59:53.048725\n",
      "Epoch 31, Loss 191.72049623377183 at 2024-10-08 11:59:55.866095\n",
      "Epoch 32, Loss 191.66573528214997 at 2024-10-08 11:59:58.579632\n",
      "Epoch 33, Loss 191.64467067344515 at 2024-10-08 12:00:01.363417\n",
      "Epoch 34, Loss 191.56983379289215 at 2024-10-08 12:00:04.121319\n",
      "Epoch 35, Loss 191.5526690015606 at 2024-10-08 12:00:06.788940\n",
      "Epoch 36, Loss 191.52240409103095 at 2024-10-08 12:00:09.464899\n",
      "Epoch 37, Loss 191.4746862673292 at 2024-10-08 12:00:12.084490\n",
      "Epoch 38, Loss 191.47459381701898 at 2024-10-08 12:00:14.723827\n",
      "Epoch 39, Loss 191.4220987955729 at 2024-10-08 12:00:17.276774\n",
      "Epoch 40, Loss 191.3974459779029 at 2024-10-08 12:00:19.971688\n",
      "Epoch 41, Loss 191.33722133262484 at 2024-10-08 12:00:22.841422\n",
      "Epoch 42, Loss 191.34557028377756 at 2024-10-08 12:00:25.508940\n",
      "Epoch 43, Loss 191.3258804620481 at 2024-10-08 12:00:28.071349\n",
      "Epoch 44, Loss 191.31933982699525 at 2024-10-08 12:00:30.704964\n",
      "Epoch 45, Loss 191.2955759085861 at 2024-10-08 12:00:34.095541\n",
      "Epoch 46, Loss 191.27688703349992 at 2024-10-08 12:00:37.457436\n",
      "Epoch 47, Loss 191.25637847302008 at 2024-10-08 12:00:40.355149\n",
      "Epoch 48, Loss 191.21547564338235 at 2024-10-08 12:00:43.319081\n",
      "Epoch 49, Loss 191.221710803462 at 2024-10-08 12:00:46.055827\n",
      "Bootstrap iteration: 4/5\n",
      "Loading /Users/brendamelano/Desktop/Reprogramming_Osteosarcoma/plain_scRNAseq_analysis/OS384/OS384_adata_subtype_PCA.h5ad\n",
      "Epoch 0, Loss 217.0717519124349 at 2024-10-08 12:00:49.524839\n",
      "Epoch 1, Loss 201.8691463096469 at 2024-10-08 12:00:52.391434\n",
      "Epoch 2, Loss 199.0352564793007 at 2024-10-08 12:00:55.456269\n",
      "Epoch 3, Loss 198.28190792308135 at 2024-10-08 12:00:57.991329\n",
      "Epoch 4, Loss 197.63582715801164 at 2024-10-08 12:01:00.769762\n",
      "Epoch 5, Loss 197.17925965552237 at 2024-10-08 12:01:03.623879\n",
      "Epoch 6, Loss 196.67736277860755 at 2024-10-08 12:01:06.247189\n",
      "Epoch 7, Loss 196.31910959879556 at 2024-10-08 12:01:08.807027\n",
      "Epoch 8, Loss 195.8019053141276 at 2024-10-08 12:01:11.322614\n",
      "Epoch 9, Loss 195.5291664273131 at 2024-10-08 12:01:14.289563\n",
      "Epoch 10, Loss 195.1986437030867 at 2024-10-08 12:01:16.991892\n",
      "Epoch 11, Loss 194.8098186418122 at 2024-10-08 12:01:20.296517\n",
      "Epoch 12, Loss 194.58976865282247 at 2024-10-08 12:01:25.304698\n",
      "Epoch 13, Loss 194.28490821987975 at 2024-10-08 12:01:29.153241\n",
      "Epoch 14, Loss 194.01691990272673 at 2024-10-08 12:01:32.381218\n",
      "Epoch 15, Loss 193.7635174919577 at 2024-10-08 12:01:36.145516\n",
      "Epoch 16, Loss 193.5721752690334 at 2024-10-08 12:01:39.220260\n",
      "Epoch 17, Loss 193.35357815611596 at 2024-10-08 12:01:41.983166\n",
      "Epoch 18, Loss 193.17258318732766 at 2024-10-08 12:01:44.804652\n",
      "Epoch 19, Loss 192.99357425465303 at 2024-10-08 12:01:47.577790\n",
      "Epoch 20, Loss 192.851141836129 at 2024-10-08 12:01:50.397081\n",
      "Epoch 21, Loss 192.7042828728171 at 2024-10-08 12:01:53.472744\n",
      "Epoch 22, Loss 192.57478661630668 at 2024-10-08 12:01:57.794826\n",
      "Epoch 23, Loss 192.40021589690565 at 2024-10-08 12:02:01.582716\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Main code\n",
    "\n",
    "# List of datasets\n",
    "datasets = ['OS152', 'OS384', 'OS742']\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_bootstrap = 5\n",
    "\n",
    "# Loop over each dataset\n",
    "for dataset in datasets:\n",
    "    print(f\"Processing dataset: {dataset}\")\n",
    "\n",
    "    # Define paths and parameters specific to the dataset\n",
    "    BASE_DIR = '/Users/brendamelano/Desktop/Reprogramming_Osteosarcoma/plain_scRNAseq_analysis'\n",
    "    H5PATH = f'{BASE_DIR}/{dataset}/{dataset}_adata_subtype_PCA.h5ad'\n",
    "    METAPATH = f'{BASE_DIR}/{dataset}/{dataset}_10Xbarcode_subtype.tsv'\n",
    "    GENEPATH = '/Users/brendamelano/Desktop/ciberatac/mave_data/Genes_passing_40p.txt'\n",
    "    GMTPATH = '/Users/brendamelano/Desktop/ciberatac/mave_data/c3.tft.v7.2.symbols.gmt'\n",
    "    OUTDIR = f'{BASE_DIR}/{dataset}/output'\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "    # Set parameters\n",
    "    gmtpath = GMTPATH\n",
    "    outdir = OUTDIR\n",
    "    nparpaths = [H5PATH]\n",
    "    numlvs = 10\n",
    "    genepath = GENEPATH\n",
    "    metapaths = [METAPATH]\n",
    "    num_celltypes = 3\n",
    "    predict_celltypes = True\n",
    "    use_connections = True\n",
    "    loss_scalers = [1000, 1, 1]\n",
    "    dont_train = False\n",
    "    filter_var = False\n",
    "    num_genes = 2000\n",
    "    include_batches = False\n",
    "\n",
    "    # Initialize arrays to store bootstrap weights\n",
    "    state_tf_weights_bootstrap = []\n",
    "\n",
    "    for b in range(n_bootstrap):\n",
    "        print(f\"Bootstrap iteration: {b+1}/{n_bootstrap}\")\n",
    "\n",
    "        # Run the main function and capture the returned values\n",
    "        vae, gmtmat_aligned, tfs, states = main(\n",
    "            gmtpath=gmtpath,\n",
    "            nparpaths=nparpaths,\n",
    "            outdir=outdir,\n",
    "            numlvs=numlvs,\n",
    "            metapaths=metapaths,\n",
    "            dont_train=dont_train,\n",
    "            genepath=genepath,\n",
    "            existingmodelpath='NA',\n",
    "            use_connections=use_connections,\n",
    "            loss_scalers=loss_scalers,\n",
    "            predict_celltypes=predict_celltypes,\n",
    "            num_celltypes=num_celltypes,\n",
    "            filter_var=filter_var,\n",
    "            num_genes=num_genes,\n",
    "            include_batches=include_batches\n",
    "        )\n",
    "\n",
    "        # Now vae is defined, and you can proceed to extract the model weights\n",
    "        # Extract model weights\n",
    "        model_state_dict = vae.state_dict()\n",
    "\n",
    "        # Extract classifier weights\n",
    "        classifier_weights = model_state_dict['classifier.weight'].cpu().detach().numpy()  # Shape: (n_states, n_latent)\n",
    "\n",
    "        # Extract decoder weights\n",
    "        decoder_weight_0 = model_state_dict['decoder.0.weight'].cpu().detach().numpy()  # Shape: (n_hidden, n_latent)\n",
    "        decoder_weight_3 = model_state_dict['decoder.3.weight'].cpu().detach().numpy()  # Shape: (n_input, n_hidden)\n",
    "\n",
    "        # Compute the effective decoder weight matrix\n",
    "        effective_decoder_weights = np.dot(decoder_weight_3, decoder_weight_0)  # Shape: (n_input, n_latent)\n",
    "\n",
    "        # 'gmtmat_aligned' is already aligned with 'expar' genes\n",
    "        connections = gmtmat_aligned  # Shape: (n_input, n_tfs)\n",
    "\n",
    "        # Compute TF weights\n",
    "        tf_weights = np.dot(effective_decoder_weights.T, connections)  # Shape: (n_latent, n_tfs)\n",
    "\n",
    "        # Compute influence of TFs on States\n",
    "        state_tf_weights = np.dot(classifier_weights, tf_weights)  # Shape: (n_states, n_tfs)\n",
    "\n",
    "        # Store the weights\n",
    "        state_tf_weights_bootstrap.append(state_tf_weights)\n",
    "\n",
    "    # Convert list to numpy array\n",
    "    state_tf_weights_bootstrap = np.array(state_tf_weights_bootstrap)  # Shape: (n_bootstrap, n_states, n_tfs)\n",
    "\n",
    "    # Compute mean and std deviations\n",
    "    state_tf_weights_mean = np.mean(state_tf_weights_bootstrap, axis=0)  # Shape: (n_states, n_tfs)\n",
    "    state_tf_weights_std = np.std(state_tf_weights_bootstrap, axis=0)  # Shape: (n_states, n_tfs)\n",
    "\n",
    "    # Proceed with p-value computation and plotting\n",
    "    n_states = len(states)\n",
    "    ncols = 3  # Adjust as needed\n",
    "    nrows = int(np.ceil(n_states / ncols))\n",
    "\n",
    "    subplot_width = 3  # inches\n",
    "    subplot_height = 3  # inches\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(subplot_width * ncols, subplot_height * nrows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    top_n = 10  # Number of top TFs to highlight per State\n",
    "    effect_size_cutoff = 0.1  # Adjust as needed\n",
    "    p_value_cutoff = 0.05  # Adjust as needed\n",
    "\n",
    "    # Calculate the -log10 of the p-value cutoff for plotting the hor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdbff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import scanpy as sc\n",
    "import scvelo as scv\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import scipy.stats\n",
    "\n",
    "# List of datasets\n",
    "datasets = ['OS152', 'OS384', 'OS742']\n",
    "\n",
    "# Loop over each dataset\n",
    "for dataset in datasets:\n",
    "    print(f\"Processing dataset: {dataset}\")\n",
    "    \n",
    "    # Define paths and parameters specific to the dataset\n",
    "    BASE_DIR = '/Users/brendamelano/Desktop/Reprogramming_Osteosarcoma/plain_scRNAseq_analysis'\n",
    "    H5PATH = f'{BASE_DIR}/{dataset}/{dataset}_adata_subtype_PCA.h5ad'\n",
    "    METAPATH = f'{BASE_DIR}/{dataset}/{dataset}_10Xbarcode_subtype.tsv'\n",
    "    GENEPATH = '/Users/brendamelano/Desktop/ciberatac/mave_data/Genes_passing_40p.txt'\n",
    "    GMTPATH = '/Users/brendamelano/Desktop/ciberatac/mave_data/c3.tft.v7.2.symbols.gmt'\n",
    "    OUTDIR = f'{BASE_DIR}/{dataset}/output'\n",
    "    \n",
    "    # Load your data\n",
    "    adata_path = f\"{BASE_DIR}/{dataset}/{dataset}_adata_subtype_PCA.h5ad\"\n",
    "    adata = scv.read(adata_path)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(OUTDIR, exist_ok=True)\n",
    "    \n",
    "    # Set parameters\n",
    "    gmtpath = GMTPATH\n",
    "    outdir = OUTDIR\n",
    "    nparpaths = [H5PATH]\n",
    "    numlvs = 10\n",
    "    genepath = GENEPATH\n",
    "    metapaths = [METAPATH]\n",
    "    num_celltypes = 8\n",
    "    predict_celltypes = True\n",
    "    use_connections = True\n",
    "    loss_scalers = [1000, 1, 1]\n",
    "    dont_train = False\n",
    "    filter_var = False\n",
    "    num_genes = 2000\n",
    "    include_batches = False\n",
    "    \n",
    "    # Run the main function and capture the returned values\n",
    "    vae, gmtmat_aligned, tfs, states = main(\n",
    "        gmtpath=gmtpath,\n",
    "        nparpaths=nparpaths,\n",
    "        outdir=outdir,\n",
    "        numlvs=numlvs,\n",
    "        metapaths=metapaths,\n",
    "        dont_train=dont_train,\n",
    "        genepath=genepath,\n",
    "        existingmodelpath='NA',\n",
    "        use_connections=use_connections,\n",
    "        loss_scalers=loss_scalers,\n",
    "        predict_celltypes=predict_celltypes,\n",
    "        num_celltypes=num_celltypes,\n",
    "        filter_var=filter_var,\n",
    "        num_genes=num_genes,\n",
    "        include_batches=include_batches\n",
    "    )\n",
    "    \n",
    "    # Now vae is defined, and you can proceed to extract the model weights\n",
    "    # Extract model weights\n",
    "    model_state_dict = vae.state_dict()\n",
    "    \n",
    "    # Extract classifier weights\n",
    "    classifier_weights = model_state_dict['classifier.weight'].cpu().detach().numpy()  # Shape: (n_states, n_latent)\n",
    "    \n",
    "    # Extract decoder weights\n",
    "    decoder_weight_0 = model_state_dict['decoder.0.weight'].cpu().detach().numpy()  # Shape: (n_hidden, n_latent)\n",
    "    decoder_weight_3 = model_state_dict['decoder.3.weight'].cpu().detach().numpy()  # Shape: (n_input, n_hidden)\n",
    "    \n",
    "    # Compute the effective decoder weight matrix\n",
    "    effective_decoder_weights = np.dot(decoder_weight_3, decoder_weight_0)  # Shape: (n_input, n_latent)\n",
    "    \n",
    "    # 'gmtmat_aligned' is already aligned with 'expar' genes\n",
    "    connections = gmtmat_aligned  # Shape: (n_input, n_tfs)\n",
    "    \n",
    "    # Compute TF weights\n",
    "    tf_weights = np.dot(effective_decoder_weights.T, connections)  # Shape: (n_latent, n_tfs)\n",
    "    \n",
    "    # Compute influence of TFs on States\n",
    "    state_tf_weights = np.dot(classifier_weights, tf_weights)  # Shape: (n_states, n_tfs)\n",
    "    \n",
    "    # Normalize the weights\n",
    "    state_tf_weights_norm = state_tf_weights / np.linalg.norm(state_tf_weights, axis=1, keepdims=True)\n",
    "    \n",
    "    # Get TF names\n",
    "    tf_names = tfs  # List of TF names\n",
    "    \n",
    "    # Plotting with FDR correction\n",
    "    n_states = len(states)\n",
    "    ncols = 3  # Adjust as needed\n",
    "    nrows = int(np.ceil(n_states / ncols))\n",
    "    \n",
    "    subplot_width = 3  # inches\n",
    "    subplot_height = 3  # inches\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(subplot_width * ncols, subplot_height * nrows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    top_n = 10  # Number of top TFs to highlight per State\n",
    "    effect_size_cutoff = 0.1  # Adjust as needed\n",
    "    p_value_cutoff = 0.05  # Adjust as needed\n",
    "    \n",
    "    # Calculate the -log10 of the p-value cutoff for plotting the horizontal line\n",
    "    neg_log_p_value_cutoff = -np.log10(p_value_cutoff)\n",
    "    \n",
    "    for i, state in enumerate(states):\n",
    "        state_weights = state_tf_weights_norm[i]\n",
    "        tf_weights = state_weights  # All TF weights for the state\n",
    "        tf_names_state = tf_names  # All TF names\n",
    "        \n",
    "        # Compute p-values\n",
    "        std_dev = np.std(state_weights) if np.std(state_weights) > 0 else 1\n",
    "        df = len(state_weights) - 1\n",
    "        t_stats = tf_weights / (std_dev / np.sqrt(len(state_weights)))\n",
    "        p_values = 2 * (1 - scipy.stats.t.cdf(np.abs(t_stats), df))\n",
    "        \n",
    "        # Apply FDR correction\n",
    "        corrected_p_values = multipletests(p_values, method='fdr_bh')[1]\n",
    "        \n",
    "        # Create a volcano plot\n",
    "        ax = axes[i]\n",
    "        effect_sizes = tf_weights  # Effect sizes on x-axis\n",
    "        log_p_values = -np.log10(corrected_p_values)  # -log10(p-values) on y-axis\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax.scatter(effect_sizes, log_p_values, color='grey', alpha=0.7)\n",
    "        \n",
    "        # Determine which TFs to highlight\n",
    "        significant = (np.abs(effect_sizes) >= effect_size_cutoff) & (corrected_p_values <= p_value_cutoff)\n",
    "        top_tf_indices = np.argsort(-np.abs(effect_sizes))[:top_n]\n",
    "        highlight_indices = np.where(significant)[0]\n",
    "        # Combine top N and significant indices\n",
    "        highlight_indices = np.unique(np.concatenate([top_tf_indices, highlight_indices]))\n",
    "        \n",
    "        # Highlight and annotate the selected TFs\n",
    "        ax.scatter(effect_sizes[highlight_indices], log_p_values[highlight_indices], color='red', alpha=0.8)\n",
    "        for idx in highlight_indices:\n",
    "            tf_name = tf_names_state[idx]\n",
    "            x = effect_sizes[idx]\n",
    "            y = log_p_values[idx]\n",
    "            ax.annotate(tf_name, (x, y), textcoords=\"offset points\", xytext=(0,5), ha='center', fontsize=8)\n",
    "        \n",
    "        # Add the horizontal dotted red line at the significance cutoff\n",
    "        ax.axhline(y=neg_log_p_value_cutoff, color='red', linestyle='--', linewidth=1)\n",
    "        ax.text(ax.get_xlim()[1], neg_log_p_value_cutoff, f'p = {p_value_cutoff}', color='red', va='bottom', ha='right', fontsize=8)\n",
    "        \n",
    "        # Set titles and labels\n",
    "        ax.set_title(f\"State {state} in Dataset {dataset}\")\n",
    "        ax.set_xlabel(\"Effect Size (Normalized Weight)\")\n",
    "        ax.set_ylabel(\"-log10(p-value)\")\n",
    "    \n",
    "    # Remove any empty subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure as SVG to desktop\n",
    "    desktop_path = '/Users/brendamelano/Desktop/'\n",
    "    output_filename = f'{desktop_path}{dataset}_state_tf_volcano_plot.svg'\n",
    "    plt.savefig(output_filename, format='svg')\n",
    "    print(f\"Plot saved to {output_filename}\")\n",
    "    \n",
    "    plt.close(fig)  # Close the figure to prevent overlap in the next iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a43dfac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
